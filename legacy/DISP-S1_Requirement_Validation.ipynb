{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow to Validate NISAR L2 Secular Displacement Requirement\n",
    "\n",
    "**Original code authored by:** David Bekaert, Heresh Fattahi, Eric Fielding, and Zhang Yunjun with \n",
    "Extensive modifications by Adrian Borsa and Amy Whetter and other NISAR team members 2022\n",
    "\n",
    "**Updated for OPERA requirements by Simran Sangha, Marin Govorcin, and Al Handwerger**\n",
    "\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "Both the initial setup (<b>Prep A</b> section) and download of the data (<b>Prep B</b> section) should be run at the start of the notebook. And all subsequent sections NEED to be run in order.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define CalVal Site "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "### Choose a site from the 'sites' dictionary found 2 cells down\n",
    "## If your study area is not defined, add a new dictionary entry as appropriate and provide a unique site keyname\n",
    "site = 'CentralValleyD144_4yr'\n",
    "\n",
    "# set path to aria-tools/MintPy/FRInGE output (if already generated)\n",
    "work_dir = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define credentials\n",
    "earthdata_user = ''\n",
    "earthdata_password = ''\n",
    "opentopography_api_key = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define list of requirements\n",
    "## Static for OPERA Cal/Val requirements, do not touch!\n",
    "\n",
    "findMax = 'true' # set to 'true' if you want to find the maximum threshold, set to 'false' if you want to find the minimum threshold\n",
    "\n",
    "# Define secular requirements\n",
    "secular_gnss_rqmt = 3  # mm/yr for 3 years of data over length scales of 0.1-50 km\n",
    "gnss_dist_rqmt = [0.1, 50.0]  # km\n",
    "secular_insar_rqmt = 3  # mm/yr\n",
    "insar_dist_rqmt = [0.1, 50.0]  # km\n",
    "\n",
    "# Define temporal sampling requirement\n",
    "insar_sampling = 12 # days\n",
    "insar_sampling_percentage = 80 # percentage of acquitions at 12 day sampling (insar_sampling) or better\n",
    "insar_timespan_requirement = 4 # years\n",
    "\n",
    "# Define spatial coherence threshold (necessary to reject poor quality, long temporal baseline pairs)\n",
    "coherenceBased_parm = 'yes'\n",
    "minCoherence_parm = 0.4\n",
    "\n",
    "# Set mask file\n",
    "maskFile = 'maskTempCoh.h5' # maskTempCoh.h5 maskConnComp.h5 waterMask.h5 (maskConnComp.h5 is very conservative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### List of CalVal Sites:\n",
    "## Set maskWater flag to True if you are not working with AO restricted to within a continental interior\n",
    "'''\n",
    "Set NISAR calval sites:\n",
    "    CentralValleyD144  : Central Valley - Descending track\n",
    "    CentralValleyA137  : Central Valley - Ascending track\n",
    "\n",
    "ARIA & MintPy parameters:\n",
    "    calval_location : name\n",
    "    download_region : download box in S,N,W,E format\n",
    "    analysis_region : analysis box in S,N,W,E format (must be within download_region)\n",
    "    reference_lalo : latitute,longitude in geographic coordinates (default: auto)\n",
    "    download_start_date : download start date as YYYMMDD  \n",
    "    download_end_date   : download end date as YYYMMDD\n",
    "    sentinel_track : sentinel track to download\n",
    "    gps_ref_site_name : Name of the GPS site for InSAR re-referencing\n",
    "    tempBaseMax' : maximum number of days, 'don't use interferograms longer than this value \n",
    "    ifgExcludeList : default is not to exclude any interferograms\n",
    "    earthquakeDate' : specify date of EQ step/volcanic eruption. if not applicable, specify is auto which is the default\n",
    "    maskWater' :  interior locations don't need to mask water\n",
    "'''\n",
    "sites = {\n",
    "    ##########  CENTRAL VALLEY descending ##############\n",
    "    'CentralValleyD144_4yr' : {'calval_location' : 'CentralValleyD144_4yr',\n",
    "            'download_region' : '\"36.18 36.26 -119.91 -119.77\"', # download box in S,N,W,E format\n",
    "            'analysis_region' : '\"35.77 36.75 -120.61 -118.06\"', # analysis box in S,N,W,E format (must be within download_region)\n",
    "            'reference_lalo' : 'auto',\n",
    "            'download_start_date' : '20180101',\n",
    "            'download_end_date' : '20220101',\n",
    "            'sentinel_track' : '144',\n",
    "            'gps_ref_site_name' : 'P056', # reference site for this area\n",
    "            'tempBaseMax' : 'auto',\n",
    "            'ifgExcludeList' : 'auto',\n",
    "            'earthquakeDate' : '20190705', # time for Ridgecrest EQ. Change to `auto` to toggle off if there is no deformation event\n",
    "            'maskWater' : False,\n",
    "            'reference_mask' : 'auto',\n",
    "            'use_staged_data': False, # option to control the use of pre-staged data; [False/True]\n",
    "            'use_mintpy': True, # specify use of MintPy (applicable to all cases aside for FRInGE); [False/True]\n",
    "            'vmin' : -100,\n",
    "            'vmax' : 100},\n",
    "    ##########  CENTRAL VALLEY ascending ##############\n",
    "    'CentralValleyA137_4yr' : {'calval_location' : 'CentralValleyA137_4yr',\n",
    "            'download_region' : '\"36.18 36.26 -119.91 -119.77\"', # download box in S,N,W,E format\n",
    "            'analysis_region' : '\"35.77 36.75 -120.61 -118.06\"', # analysis box in S,N,W,E format (must be within download_region)\n",
    "            'reference_lalo' : 'auto',\n",
    "            'download_start_date' : '20180101',\n",
    "            'download_end_date' : '20220101',\n",
    "            'sentinel_track' : '137',\n",
    "            'gps_ref_site_name' : 'P056', # reference site for this area\n",
    "            'tempBaseMax' : 'auto',\n",
    "            'ifgExcludeList' : 'auto',\n",
    "            'earthquakeDate' : '20190705', # time for Ridgecrest EQ. Change to `auto` to toggle off if there is no deformation event\n",
    "            'maskWater' : False,\n",
    "            'reference_mask' : 'auto',\n",
    "            'use_staged_data': False, # option to control the use of pre-staged data; [False/True]\n",
    "            'use_mintpy': True, # specify use of MintPy (applicable to all cases aside for FRInGE); [False/True]\n",
    "            'vmin' : -100,\n",
    "            'vmax' : 100},\n",
    "    ##########  Mojave descending ##############\n",
    "    'MojaveD173_4year': {'calval_location' : 'MojaveD173_4year',\n",
    "            'download_region' : '\"34.5 35.6 -116.62 -114.39\"', # download box in S,N,W,E format\n",
    "            'analysis_region' : '\"34.66 35.60 -116.62 -114.39\"', # analysis box in S,N,W,E format (must be within download_region)\n",
    "            'reference_lalo' : '35.20495,-115.81229',\n",
    "            'download_start_date' : '20180101',\n",
    "            'download_end_date' : '20220101',\n",
    "            'sentinel_track' : '173',\n",
    "            'gps_ref_site_name' : 'P619', # reference site for this area\n",
    "            'tempBaseMax' : 'auto',\n",
    "            'ifgExcludeList' : 'auto',\n",
    "            'earthquakeDate' : '20190705', # time for Ridgecrest EQ. Change to `auto` to toggle off if there is no deformation event\n",
    "            'maskWater' : False,\n",
    "            'reference_mask' : 'auto',\n",
    "            'use_staged_data': False, # option to control the use of pre-staged data; [False/True]\n",
    "            'use_mintpy': True, # specify use of MintPy (applicable to all cases aside for FRInGE); [False/True]\n",
    "            'vmin' : -20,\n",
    "            'vmax' : 20},\n",
    "    ##########  Mojave descending ##############\n",
    "    'MojaveA166_4year': {'calval_location' : 'MojaveA166_4year',\n",
    "            'download_region' : '\"34.5 35.6 -116.62 -114.39\"', # download box in S,N,W,E format\n",
    "            'analysis_region' : '\"34.66 35.60 -116.62 -114.39\"', # analysis box in S,N,W,E format (must be within download_region)\n",
    "            'reference_lalo' : '35.20495,-115.81229',\n",
    "            'download_start_date' : '20180101',\n",
    "            'download_end_date' : '20220101',\n",
    "            'sentinel_track' : '166',\n",
    "            'gps_ref_site_name' : 'P611',\n",
    "            'tempBaseMax' : 'auto',\n",
    "            'ifgExcludeList' : 'auto',\n",
    "            'earthquakeDate' : '20190705', # time for Ridgecrest EQ. Change to `auto` to toggle off if there is no deformation event\n",
    "            'maskWater' : False,\n",
    "            'reference_mask' : 'no',\n",
    "            'use_staged_data': False, # option to control the use of pre-staged data; [False/True]\n",
    "            'use_mintpy': True, # specify use of MintPy (applicable to all cases aside for FRInGE); [False/True]\n",
    "            'vmin' : -20,\n",
    "            'vmax' : 20},\n",
    "    ##########  Descending ##############\n",
    "    'HoustonD143' : {'calval_location' : 'HoustonD143',\n",
    "            'download_region' : '\"29.120047 29.935538 -95.864902 -94.555116\"', # download box in S,N,W,E format\n",
    "            'analysis_region' : '\"29.120047 29.935538 -95.864902 -94.555116\"', # analysis box in S,N,W,E format (must be within download_region)\n",
    "            'reference_lalo' : '29.757039,-95.355618',\n",
    "            'download_start_date' : '20160101',\n",
    "            'download_end_date' : '20200101',         \n",
    "            'sentinel_track' : '143',\n",
    "            'gps_ref_site_name' : 'UH01', # reference site for this area\n",
    "            'tempBaseMax' : 'auto',\n",
    "            'ifgExcludeList' : 'auto',\n",
    "            'earthquakeDate' : 'auto', # time for Ridgecrest EQ. Change to `auto` to toggle off if there is no deformation event\n",
    "            'use_staged_data': False, # option to control the use of pre-staged data; [False/True]\n",
    "            'use_mintpy': True, # specify use of MintPy (applicable to all cases aside for FRInGE); [False/True]\n",
    "            'maskWater' : True},\n",
    "    ##########  Ascending ##############\n",
    "    'HoustonA034' : {'calval_location' : 'HoustonA034',\n",
    "            'download_region' : '\"29.120047 29.935538 -95.864902 -94.555116\"', # download box in S,N,W,E format\n",
    "            'analysis_region' : '\"29.120047 29.935538 -95.864902 -94.555116\"', # analysis box in S,N,W,E format (must be within download_region)\n",
    "            'reference_lalo' : '29.757039,-95.355618',\n",
    "            'download_start_date' : '20160101',\n",
    "            'download_end_date' : '20200101',\n",
    "            'sentinel_track' : '34',\n",
    "            'gps_ref_site_name' : 'UH01', # reference site for this area\n",
    "            'tempBaseMax' : 'auto',\n",
    "            'ifgExcludeList' : 'auto',\n",
    "            'earthquakeDate' : 'auto', # time for Ridgecrest EQ. Change to `auto` to toggle off if there is no deformation event\n",
    "            'use_staged_data': False, # option to control the use of pre-staged data; [False/True]\n",
    "            'use_mintpy': True, # specify use of MintPy (applicable to all cases aside for FRInGE); [False/True]\n",
    "            'maskWater' : True}\n",
    "}\n",
    "secular_available_sites = list(sites.keys())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Table of Contents:\n",
    "<a id='secular_TOC'></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "[**Prep A. Environment Setup**](#secular_prep_a)\n",
    "\n",
    "[**Prep B. Data Staging**](#secular_prep_b)\n",
    "\n",
    "[**1. Generate Interferogram Stack**](#secular_gen_ifg)\n",
    "- [1.1.  Crop Interferograms](#secular_crop_ifg)\n",
    "\n",
    "[**2. Generation of Time Series from Interferograms**](#secular_gen_ts)\n",
    "- [2.1. Set Up MintPy Configuration file](#secular_setup_config)\n",
    "- [2.2. Load Data into MintPy](#secular_load_data)\n",
    "- [2.3. Validate/Modify Interferogram Network](#secular_validate_network)\n",
    "- [2.4. Generate Quality Control Mask](#secular_generate_mask)\n",
    "- [2.5. Reference Interferograms To Common Lat/Lon](#secular_common_latlon)\n",
    "- [2.6. Invert for SBAS Line-of-Sight Timeseries](#secular_invert_SBAS)\n",
    "\n",
    "[**3. Optional Corrections**](#secular_opt_correction)\n",
    "- [3.1. Solid Earth Tide Correction](#secular_solid_earth)\n",
    "- [3.2. Tropospheric Delay Correction](#secular_tropo_corr)\n",
    "- [3.3. Phase Deramping ](#secular_phase_deramp)\n",
    "- [3.4. Topographic Residual Correction ](#secular_topo_corr) \n",
    "\n",
    "[**4. Estimate InSAR and GNSS Velocities**](#secular_decomp_ts)\n",
    "- [4.1. Estimate InSAR LOS Velocities](#secular_insar_vel1)\n",
    "- [4.2. Find Collocated GNSS Stations](#secular_co_gps)  \n",
    "- [4.3. Get GNSS Position Time Series](#secular_gps_ts) \n",
    "- [4.4. Make GNSS LOS Velocities](#secular_gps_los)\n",
    "- [4.5. Re-Reference GNSS and InSAR Velocities](#secular_gps_insar)\n",
    "\n",
    "[**5. NISAR Validation Approach 1: GNSS-InSAR Direct Comparison**](#secular_nisar_validation)\n",
    "- [5.1. Make Velocity Residuals at GNSS Locations](#secular_make_vel)\n",
    "- [5.2. Make Double-differenced Velocity Residuals](#secular_make_velres)\n",
    "- [5.3. Secular Requirement Validation: Method 1](#secular_valid_method1)\n",
    "\n",
    "[**6. NISAR Validation Approach 2: InSAR-only Structure Function**](#secular_nisar_validation2)\n",
    "- [6.1. Read Array and Mask Pixels with no Data](#secular_array_mask)\n",
    "- [6.2. Randomly Sample Pixels and Pair Them Up with Option to Remove Trend](#secular_remove_trend)\n",
    "- [6.3. Amplitude vs. Distance of Relative Measurements (pair differences)](#secular_M2ampvsdist2)\n",
    "- [6.4. Bin Sample Pairs by Distance Bin and Calculate Statistics](#secular_M2RelMeasTable)\n",
    "\n",
    "[**Appendix: Supplementary Comparisons and Plots**](#secular_appendix1)\n",
    "- [A.1. Compare Raw Velocities](#secular_compare_raw)\n",
    "- [A.2. Plot Velocity Residuals](#secular_plot_vel)\n",
    "- [A.3. Plot Double-differenced Residuals](#secular_plot_velres)\n",
    "- [A.4. GPS Position Plot](#secular_appendix_gps)\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_prep_a'></a>\n",
    "## Prep A. Environment Setup\n",
    "Setup your environment for processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Packages\n",
    "import glob\n",
    "import os\n",
    "import subprocess\n",
    "from datetime import datetime as dt\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from mintpy.objects import gps, timeseries\n",
    "from mintpy.objects.gps import search_gps, GPS\n",
    "from mintpy.smallbaselineApp import TimeSeriesAnalysis\n",
    "from mintpy.utils import ptime, readfile, utils as ut\n",
    "from mintpy.cli import view, plot_network\n",
    "from scipy import signal\n",
    "\n",
    "from solid_utils.sampling import load_geo, samp_pair, profile_samples, haversine_distance\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "#Set Global Plot Parameters\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "################# Set Directories ##########################################\n",
    "print('\\nCurrent directory:',os.getcwd())\n",
    "\n",
    "work_dir = Path(work_dir)\n",
    "\n",
    "print(\"Work directory:\", work_dir)\n",
    "work_dir.mkdir(parents=True, exist_ok=True)\n",
    "# Change to Workdir\n",
    "os.chdir(work_dir)\n",
    "\n",
    "gunw_dir = work_dir/'products'\n",
    "gunw_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(\"   GUNW    dir:\", gunw_dir) \n",
    "\n",
    "mintpy_dir = work_dir/'MintPy' \n",
    "mintpy_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(\"   MintPy  dir:\", mintpy_dir)\n",
    "### Change to MintPy workdir\n",
    "os.chdir(mintpy_dir)\n",
    "vel_file = os.path.join(mintpy_dir, 'velocity.h5')\n",
    "msk_file = os.path.join(mintpy_dir, maskFile)  # maskTempCoh.h5 maskConnComp.h5 waterMask.h5\n",
    "\n",
    "if site not in secular_available_sites:\n",
    "    msg = '\\nSelected site not available! Please select one of the following sites:: \\n{}'.format(secular_available_sites)\n",
    "    raise Exception(msg)\n",
    "else:\n",
    "    print('\\nSelected site: {}'.format(site))\n",
    "    for key, value in sites[site].items():\n",
    "        print('   '+ key, ' : ', value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_prep_b'></a>\n",
    "## Prep B. Data Staging\n",
    "\n",
    "In this initial processing step, all the necessary Level-2 unwrapped interferogram products are gathered, organized and reduced to a common grid for analysis with MintPy. Ascending and descending stacks of nearest-neighbor and skip-1 interferograms will be prepared for independent analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### DO NOT CHANGE LINES BELOW ########\n",
    "\n",
    "##################### 1. Download (Aria) Interferograms from ASF ################\n",
    "os.chdir(work_dir)\n",
    "# do not re-download products\n",
    "if os.path.exists('products'):\n",
    "    if not os.listdir('products'):\n",
    "        print('NEEDED To Download ARIA GUNWs: \\n Link to create account : https://urs.earthdata.nasa.gov/')\n",
    "        print('NEEDED To Download DEMs: \\n Link to create account : https://portal.opentopography.org/login')\n",
    "        if os.path.exists('~/.topoapi'): # if OpenTopo API key already installed\n",
    "            print('OpenTopo API key appears to be installed, using that')\n",
    "        else:\n",
    "            print('API key location: My Account > myOpenTopo Authorizations and API Key > Request API key')\n",
    "            #opentopography_api_key = input('Please type your OpenTopo API key:')\n",
    "\n",
    "        ######################## USE ARIA-TOOLS TO DOWNLOAD GUNW ########################\n",
    "        '''\n",
    "        REFERENCE: https://github.com/aria-tools/ARIA-tools\n",
    "        '''\n",
    "        aria_download = '''ariaDownload.py -b {bbox} -u {user} -p {password} -s {start}  -e {end} -t {track} -o Count'''\n",
    "\n",
    "        ###############################################################################\n",
    "        print('CalVal site {}'.format(site))\n",
    "        print('  Searching for available GUNW products:\\n')\n",
    "\n",
    "        command = aria_download.format(bbox = sites[site]['download_region'],\n",
    "                                    start = sites[site]['download_start_date'],\n",
    "                                    end = sites[site]['download_end_date'],\n",
    "                                    track = sites[site]['sentinel_track'],\n",
    "                                    user = earthdata_user,\n",
    "                                    password = earthdata_password)\n",
    "        \n",
    "        process = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text = True, shell = True)\n",
    "        print(process.stdout)\n",
    "\n",
    "        ############## Download GUNW ##################\n",
    "        print(\"Start downloading GUNW files ...\")\n",
    "        process = subprocess.run(command.split(' -o')[0], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, shell=True)\n",
    "        # Missing progressbar\n",
    "        print(\"Downloaded {} GUNW files in: {}\\n\".format(len([(x) for x in os.listdir(gunw_dir) if x.endswith('.nc')]), gunw_dir))\n",
    "\n",
    "        ############## DO little CLEANING ###########\n",
    "        data_to_clean = [\"avg_rates.csv\", \"ASFDataDload0.py\", \"AvgDlSpeed.png\", \"error.log\"]\n",
    "\n",
    "        for i, file in enumerate(data_to_clean):\n",
    "            print('Cleaning unnecessary data {} in {}'.format(file, gunw_dir))\n",
    "            (gunw_dir/file).unlink(missing_ok=True)\n",
    "\n",
    "        #Delete error log file from workdir\n",
    "        print('Cleaning unnecessary data error.log in {}'.format(work_dir))\n",
    "        (work_dir/\"error.log\").unlink(missing_ok=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_gen_ifg'></a>\n",
    "# 1. Generate Interferogram Stack\n",
    "\n",
    "InSAR time series (i.e., the unfiltered displacement of each pixel vs. time) are estimated from a processed InSAR stack from Section 3.1 (either ascending or descending) using a variant of the small baseline subset (SBAS) approach and then parameterized using the approach described in Section 4. This step uses tools available in the MintPy software package (REF), which provides both SBAS time series and model-based time series parameterization. Recent results on InSAR closure phase and “fading signal” recommend the of use all available interferograms to avoid systematic bias (_Ansari et al._, 2020; _Zheng Y.J. et al._, 2022). As we expect high-quality orbital control for NISAR, we anticipate that the interferogram stack will typically include all nearest-neighbor (i.e., ~12-day pairs) and skip-1 interferograms, which will be the minimum inputs into the SBAS generation step.\n",
    "\n",
    "We use the open-source ARIA-tools package to download processed L2 interferograms over selected cal/val regions from the Alaska Satellite Facility archive and to stitch/crop the frame-based NISAR GUNW products to stacks that can be directly ingested into MintPy for time-series processing. ARIA-tools uses a phase-minimization approach in the product overlap region to stitch the unwrapped and ionospheric phase, a mosaicing approach for coherence and amplitude, and extracts the geometric information from the 3D data cubes through a mosaicking of the 3D datacubes and subsequent intersection with a DEM. ARIA has been used to pre-process NISAR beta products derived from Sentinel-1 which have revealed interseismic deformation and creep along the San Andreas Fault system, along with subsidence, landsliding, and other signals. \n",
    "\n",
    "We use MintPy to validate and modify the InSAR stack, removing interferograms that do not meet minimum coherence criteria, generating a quality control mask for the purpose of identifying noisy pixels within the stack, and referencing estimated deformation to a common location in all interferograms."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_crop_ifg'></a>\n",
    "## 1.1. Crop Interferograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop Interferograms to Analysis Region\n",
    "mask_file = 'auto'\n",
    "if not sites[site]['use_staged_data']:\n",
    "    ###########################################################################################################\n",
    "    # Set up ARIA product and mask data with GSHHS water mask:\n",
    "    '''\n",
    "    REQUIRED: Acquire API key to access/download DEMs\n",
    "\n",
    "    Follow instructions listed here to generate and access API key through OpenTopography:\n",
    "    https://opentopography.org/blog/introducing-api-keys-access-opentopography-global-datasets.\n",
    "    '''\n",
    "    \n",
    "    if not os.path.exists(work_dir/'stack'):\n",
    "        if not os.path.exists('~/.topoapi'): # if OpenTopo API key not already installed\n",
    "            if 'opentopography_api_key' not in locals():\n",
    "                print('API key location: My Account > myOpenTopo Authorizations and API Key > Request API key')\n",
    "                #opentopography_api_key = input('Please type your OpenTopo API key:')\n",
    "            os.system('echo \"{api_key}\" > ~/.topoapi; chmod 600 ~/.topoapi'.format(api_key = str(opentopography_api_key)))\n",
    "        print('Preparing GUNWs for MintPY....')\n",
    "        if sites[site]['maskWater']:\n",
    "            mask_file = '../mask/watermask.msk'\n",
    "            command = 'ariaTSsetup.py -f \"products/*.nc\" -b ' + sites[site]['analysis_region'] + ' --mask Download  --croptounion --verbose' # slow\n",
    "        else: # skip slow mask download when we don't need to mask water\n",
    "            command = 'ariaTSsetup.py -f \"products/*.nc\" -b ' + sites[site]['analysis_region'] + ' --croptounion --verbose'\n",
    "\n",
    "        ################################## CROP & PREPARE STACK ###################################################\n",
    "        print(command)\n",
    "        result = subprocess.run(command, stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT, text=True, shell=True)\n",
    "    print('Finish preparing GUNWs for MintPy!!')\n",
    "\n",
    "####################################################################\n",
    "### Change to MintPy workdir\n",
    "os.chdir(mintpy_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_gen_ts'></a>\n",
    "# 2. Generation of Time Series from Interferograms\n",
    "\n",
    "InSAR time series (i.e., the unfiltered displacement of each pixel vs. time) are estimated from a processed InSAR stack from Section 3.1 (either ascending or descending) using a variant of the small baseline subset (SBAS) approach and then parameterized using the approach described in Section 4. This step uses tools available in the MintPy software package (REF), which provides both SBAS time series and model-based time series parameterization. Recent results on InSAR closure phase and “fading signal” recommend the of use all available interferograms to avoid systematic bias (_Ansari et al._, 2020; _Zheng Y.J. et al._, 2022). As we expect high-quality orbital control for NISAR, we anticipate that the interferogram stack will typically include all nearest-neighbor (i.e., ~12-day pairs) and skip-1 interferograms, which will be the minimum inputs into the SBAS generation step.\n",
    "\n",
    "We use the open-source ARIA-tools package to download processed L2 interferograms over selected cal/val regions from the Alaska Satellite Facility archive and to stitch/crop the frame-based NISAR GUNW products to stacks that can be directly ingested into MintPy for time-series processing. ARIA-tools uses a phase-minimization approach in the product overlap region to stitch the unwrapped and ionospheric phase, a mosaicing approach for coherence and amplitude, and extracts the geometric information from the 3D data cubes through a mosaicking of the 3D datacubes and subsequent intersection with a DEM. ARIA has been used to pre-process NISAR beta products derived from Sentinel-1 which have revealed interseismic deformation and creep along the San Andreas Fault system, along with subsidence, landsliding, and other signals. \n",
    "\n",
    "We use MintPy to validate and modify the InSAR stack, removing interferograms that do not meet minimum coherence criteria, generating a quality control mask for the purpose of identifying noisy pixels within the stack, and referencing estimated deformation to a common location in all interferograms."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_setup_config'></a>\n",
    "## 2.1. Set Up MintPy Configuration file\n",
    "\n",
    "\n",
    "The default processing parameters for MintPy's **smallbaselineApp.py** need to be modified by including the following lines in config_file (which must be manually created and placed into mint_dir):\n",
    "\n",
    "- mintpy.load.processor      = aria\n",
    "- mintpy.load.unwFile        = ../stack/unwrapStack.vrt\n",
    "- mintpy.load.corFile        = ../stack/cohStack.vrt\n",
    "- mintpy.load.connCompFile   = ../stack/connCompStack.vrt\n",
    "- mintpy.load.demFile        = ../DEM/SRTM_3arcsec.dem\n",
    "- mintpy.load.incAngleFile   = ../incidenceAngle/{download_start_date}_{download_edn_date}.vrt\n",
    "- mintpy.load.azAngleFile    = ../azimuthAngle/{download_start_date}_{download_edn_date}.vrt\n",
    "- mintpy.load.waterMaskFile  = ../mask/watermask.msk\n",
    "- mintpy.reference.lalo      = auto, or somewhere in your bounding box\n",
    "- mintpy.topographicResidual.pixelwiseGeometry = no\n",
    "- mintpy.troposphericDelay.method              = no\n",
    "- mintpy.topographicResidual                   = no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config_file = Path(mintpy_dir)/(sites[site]['calval_location'] + '.cfg')\n",
    "\n",
    "dem_file = glob.glob('../DEM/*dem')[0]\n",
    "\n",
    "if not sites[site]['use_staged_data'] and sites[site]['use_mintpy'] is True:\n",
    "    ####################################################################\n",
    "    ### Write smallbaseline.py config file\n",
    "    config_file_content = \"\"\"\n",
    "    mintpy.load.processor = aria\n",
    "    mintpy.compute.numWorker = auto\n",
    "    mintpy.load.unwFile = ../stack/unwrapStack.vrt\n",
    "    mintpy.load.corFile = ../stack/cohStack.vrt\n",
    "    mintpy.load.connCompFile = ../stack/connCompStack.vrt\n",
    "    mintpy.load.demFile = {dem_file}\n",
    "    mintpy.load.incAngleFile = ../incidenceAngle/*.vrt\n",
    "    mintpy.load.azAngleFile = ../azimuthAngle/*.vrt\n",
    "    mintpy.load.waterMaskFile = {mask_file}\n",
    "    mintpy.networkInversion.minTempCoh = 0.5\n",
    "    mintpy.topographicResidual.pixelwiseGeometry = no\n",
    "    mintpy.troposphericDelay.method = no\n",
    "    mintpy.topographicResidual = no\n",
    "    mintpy.network.tempBaseMax = {tempmax}\n",
    "    mintpy.network.startDate = {startdatenet}\n",
    "    mintpy.network.endDate = {enddatenet}\n",
    "    mintpy.network.coherenceBased = {coherenceBased}\n",
    "    mintpy.network.minCoherence = {minCoherence}\n",
    "    mintpy.timeFunc.startDate = {startdatevel}\n",
    "    mintpy.timeFunc.endDate = {enddatevel}\n",
    "    mintpy.timeFunc.stepDate = auto\n",
    "    mintpy.reference.lalo = {reference_lalo}\n",
    "    mintpy.reference.maskFile = {reference_mask}\n",
    "    mintpy.network.excludeIfgIndex = {excludeIfg}\"\"\".format(dem_file = dem_file,\n",
    "                                                            mask_file = mask_file,\n",
    "                                                            tempmax=sites[site]['tempBaseMax'],\n",
    "                                                            excludeIfg=sites[site]['ifgExcludeList'],\n",
    "                                                            startdatenet=sites[site]['download_start_date'],\n",
    "                                                            enddatenet=sites[site]['download_end_date'],\n",
    "                                                            coherenceBased = coherenceBased_parm,\n",
    "                                                            minCoherence = minCoherence_parm,\n",
    "                                                            startdatevel=sites[site]['download_start_date'],\n",
    "                                                            enddatevel=sites[site]['download_end_date'],\n",
    "                                                            stepdate=sites[site]['earthquakeDate'],\n",
    "                                                            reference_lalo=sites[site]['reference_lalo'],\n",
    "                                                            reference_mask=sites[site]['reference_mask'])\n",
    "\n",
    "    config_file.write_text(config_file_content)\n",
    "\n",
    "print('MintPy config file:\\n    {}:'.format(config_file))\n",
    "print(config_file.read_text())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='secular_load_data'></a>\n",
    "## 2.2. Load Data into MintPy\n",
    "\n",
    "The output of this step is an \"inputs\" directory in 'calval_directory/calval_location/MintPy/\" containing two HDF5 files:\n",
    "- ifgramStack.h5: This file contains 6 dataset cubes (e.g. unwrapped phase, coherence, connected components etc.) and multiple metadata\n",
    "- geometryGeo.h5: This file contains geometrical datasets (e.g., incidence/azimuth angle, masks, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sites[site]['use_mintpy']:\n",
    "    command = 'smallbaselineApp.py ' + str(config_file) + ' --dostep load_data'\n",
    "    process = subprocess.run(command, shell=True)\n",
    "    print('Mintpy input files:')\n",
    "    [x for x in os.listdir('inputs') if x.endswith('.h5')]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='secular_validate_network'></a>\n",
    "## 2.3. Validate/Modify Interferogram Network\n",
    "\n",
    "Add additional parameters to config_file in order to remove selected interferograms, change minimum coherence, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sites[site]['use_mintpy']:\n",
    "    command = 'smallbaselineApp.py ' + str(config_file) + ' --dostep modify_network'\n",
    "    process = subprocess.run(command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sites[site]['use_mintpy']:\n",
    "    try:\n",
    "        plot_network.main(['inputs/ifgramStack.h5'])\n",
    "    except ValueError: # circumvents 0 bperp (https://github.com/insarlab/MintPy/issues/281)\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='secular_generate_mask'></a>\n",
    "## 2.4. Generate Quality Control Mask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mask files can be can be used to mask pixels in the time-series processing. Below we generate a mask file based on the connected components, which is a metric for unwrapping quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sites[site]['use_mintpy']:\n",
    "    command='generate_mask.py inputs/ifgramStack.h5  --nonzero  -o maskConnComp.h5  --update'\n",
    "    process = subprocess.run(command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sites[site]['use_mintpy']:\n",
    "    msk_file_concomp = os.path.join(mintpy_dir, 'maskConnComp.h5')\n",
    "    view.main([msk_file_concomp, 'mask'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='secular_common_latlon'></a>\n",
    "## 2.5. Reference Interferograms To Common Lat/Lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sites[site]['use_mintpy']:\n",
    "    command = 'smallbaselineApp.py ' + str(config_file) + ' --dostep reference_point'\n",
    "    process = subprocess.run(command, shell=True)\n",
    "    os.system('info.py inputs/ifgramStack.h5 | egrep \"REF_\"');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='secular_invert_SBAS'></a>\n",
    "## 2.6. Invert for SBAS Line-of-Sight Timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sites[site]['use_mintpy']:\n",
    "    command = 'smallbaselineApp.py ' + str(config_file) + ' --dostep invert_network'\n",
    "    process = subprocess.run(command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the time-series file used for time function estimation given the template setup\n",
    "template = readfile.read_template(os.path.join(mintpy_dir, 'smallbaselineApp.cfg'))\n",
    "template = ut.check_template_auto_value(template)\n",
    "insar_ts_file = TimeSeriesAnalysis.get_timeseries_filename(template, mintpy_dir)['velocity']['input']\n",
    "\n",
    "# Get date list\n",
    "date_list = timeseries(insar_ts_file).get_date_list()\n",
    "num_date = len(date_list)\n",
    "date0, date1 = date_list[0], date_list[-1]\n",
    "insar_dates = ptime.date_list2vector(date_list)[0]\n",
    "\n",
    "# Check temporal sampling\n",
    "insar_sampling_arr = []\n",
    "for i in range(len(insar_dates)-1):\n",
    "    diff = (insar_dates[i+1] - insar_dates[i]).days\n",
    "    insar_sampling_arr.append(diff)\n",
    "\n",
    "count = 0\n",
    "for i in insar_sampling_arr:\n",
    "    if i <= insar_sampling:\n",
    "        count += 1\n",
    "\n",
    "percentage = (count / len(insar_sampling_arr)) * 100\n",
    "timespan_of_insar=(insar_dates[len(insar_dates)-1]-insar_dates[0]).days /365.25\n",
    "\n",
    "# Overall pass/fail criterion\n",
    "if percentage >= insar_sampling_percentage:\n",
    "    print(f'This velocity dataset ({percentage}%) passes the temporal sampling requirement ({insar_sampling_percentage}%)')\n",
    "else:\n",
    "    print(f'This velocity dataset ({percentage}%) does NOT pass the temporal sampling requirement ({insar_sampling_percentage}%)')\n",
    "\n",
    "if timespan_of_insar >= insar_timespan_requirement:\n",
    "    print(f'This velocity dataset ({timespan_of_insar} years) passes the timespan requirement ({insar_timespan_requirement} years)')\n",
    "else:\n",
    "    print(f'This velocity dataset ({timespan_of_insar} years) does NOT pass the timespan requirement ({insar_timespan_requirement } years)')\n",
    "    \n",
    "       \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='secular_tropo_corr'></a>\n",
    "## 3.2. Tropospheric Delay Correction\n",
    "\n",
    "Optional atmospheric correction utilizes the PyAPS (Jolivet et al., 2011, Jolivet and Agram, 2012) module within GIAnT (or eventually a merged replacement for GIAnT and MintPy). PyAPS is well documented, maintained and can be freely downloaded. PyAPS is included in GIAnT distribution). PyAPS currently includes support for ECMWF’s ERA-Interim, NOAA’s NARR and NASA’s MERRA weather models. A final selection of atmospheric models to be used for operational NISAR processing will be done during Phase C.\n",
    "\n",
    "[T]ropospheric delay maps are produced from atmospheric data provided by Global Atmospheric Models. This method aims to correct differential atmospheric delay correlated with the topography in interferometric phase measurements. Global Atmospheric Models (hereafter GAMs)... provide estimates of the air temperature, the atmospheric pressure and the humidity as a function of elevation on a coarse resolution latitude/longitude grid. In PyAPS, we use this 3D distribution of atmospheric variables to determine the atmospheric phase delay on each pixel of each interferogram.\n",
    "\n",
    "The absolute atmospheric delay is computed at each SAR acquisition date. For a pixel a_i at an elevation z at acquisition date i, the four surrounding grid points are selected and the delays for their respective elevations are computed. The resulting delay at the pixel a_i is then the bilinear interpolation between the delays at the four grid points. Finally, we combine the absolute delay maps of the InSAR partner images to produce the differential delay maps used to correct the interferograms.\n",
    "\n",
    "[MintPy provides functionality for this correction.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "do_tropo_correction = False\n",
    "########################################################################\n",
    "'''\n",
    "REFERENCE : https://github.com/insarlab/pyaps#2-account-setup-for-era5\n",
    "Read Section 2 for ERA5 [link above] to create an account on the CDS website.\n",
    "'''\n",
    "\n",
    "if sites[site]['use_mintpy']:\n",
    "    if do_tropo_correction:\n",
    "        if not sites[site]['use_staged_data'] and not os.path.exists(Path.home()/'.cdsapirc'):\n",
    "            print('NEEDED to download ERA5, link: https://cds.climate.copernicus.eu/user/register')\n",
    "            #UID = input('Please type your CDS_UID:')\n",
    "            #CDS_API = input('Please type your CDS_API:')\n",
    "            \n",
    "            cds_tmp = '''url: https://cds.climate.copernicus.eu/api/v2\n",
    "            key: {UID}:{CDS_API}'''.format(UID=UID, CDS_API=CDS_API)\n",
    "            os.system('echo \"{cds_tmp}\" > ~/.cdsapirc; chmod 600 ~/.cdsapirc'.format(cds_tmp = str(cds_tmp)))\n",
    "        \n",
    "        command = 'smallbaselineApp.py ' + str(config_file) + ' --dostep correct_troposphere'\n",
    "        process = subprocess.run(command, shell=True)\n",
    "        \n",
    "        view.main(['inputs/ERA5.h5'])\n",
    "        timeseries_filename = 'timeseries_ERA5.h5'\n",
    "    else:\n",
    "        timeseries_filename = 'timeseries.h5'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='secular_phase_deramp'></a>\n",
    "## 3.3. Phase Deramping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sites[site]['use_mintpy']:\n",
    "    command = 'smallbaselineApp.py ' + str(config_file) + ' --dostep deramp'\n",
    "    process = subprocess.run(command, shell=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='secular_topo_corr'></a>\n",
    "## 3.4. Topographic Residual Correction \n",
    "\n",
    "[MintPy provides functionality for this correction.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sites[site]['use_mintpy']:\n",
    "    command = 'smallbaselineApp.py ' + str(config_file) + ' --dostep correct_topography'\n",
    "    process = subprocess.run(command, shell=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_decomp_ts'></a>\n",
    "# 4. Estimate InSAR and GNSS Velocities\n",
    "The approach that will be used for the generation of NISAR L3 products for Requirements 660 and 663 allows for an explicit inclusion of key basis functions (e.g., Heaviside functions, secular rate, etc.) in the InSAR inversion. Modifications to this algorithm may be identified and implemented in response to NISAR Phase C activities. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_insar_vel1'></a>\n",
    "## 4.1. Estimate InSAR LOS Velocities\n",
    "\n",
    "Given a time series of InSAR LOS displacements, the observations for a given pixel, $U(t)$, can be parameterized as:\n",
    "\n",
    "$$U(t) = a \\;+\\; vt \\;+\\; c_1 cos (\\omega_1t - \\phi_{1,}) \\;+\\; c_2 cos (\\omega_2t - \\phi_2) \\;+\\; \\sum_{j=1}^{N_{eq}} \\left( h_j+f_j F_j (t-t_j) \\right)H(t - t_j) \\;+\\; \\frac{B_\\perp (t)}{R sin \\theta}\\delta z \\;+\\; residual$$ \n",
    "\n",
    "which includes a constant offset $(a)$, velocity $(v)$, and amplitudes $(c_j)$ and phases $(\\phi_j)$ of annual $(\\omega_1)$ and semiannual $(\\omega_2)$ sinusoidal terms.  Where needed we can include additional complexity, such as coseismic and postseismic processes parameterized by Heaviside (step) functions $H$ and postseismic functions $F$ (the latter typically exponential and/or logarithmic).   $B_\\perp(t)$, $R$, $\\theta$, and $\\delta z$ are, respectively, the perpendicular component of the interferometric baseline relative to the first date, slant range distance, incidence angle and topography error correction for the given pixel. \n",
    "\n",
    "Thus, given either an ensemble of interferograms or the output of SBAS (displacement vs. time), we can write the LSQ problem as \n",
    "\n",
    "$$ \\textbf{G}\\textbf{m} = \\textbf{d}$$\n",
    "\n",
    "where $\\textbf{G}$ is the design matrix (constructed out of the different functional terms in Equation 2 evaluated either at the SAR image dates for SBAS output, or between the dates spanned by each pair for interferograms), $\\textbf{m}$ is the vector of model parameters (the coefficients in Equation 2) and $\\textbf{d}$ is the vector of observations.  For GPS time series, $\\textbf{G}, \\textbf{d}, \\textbf{m}$ are constructed using values evaluated at single epochs corresponding to the GPS solution times, as for SBAS InSAR input. \n",
    "\n",
    "With this formulation, we can obtain InSAR velocity estimates and their formal uncertainties (including in areas where the expected answer is zero). \n",
    "\n",
    "The default InSAR velocity fit in MintPy is to estimate a mean linear velocity $(v)$ in in the equation, which we do below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if sites[site]['use_mintpy']:\n",
    "    command = 'smallbaselineApp.py ' + str(config_file) + ' --dostep velocity'\n",
    "    process = subprocess.run(command, shell=True)\n",
    "\n",
    "# load velocity file\n",
    "insar_velocities,_ = readfile.read(vel_file, datasetName = 'velocity')  # read velocity file\n",
    "insar_velocities = insar_velocities * 1000.  # convert velocities from m to mm\n",
    "\n",
    "# set masked pixels to NaN\n",
    "msk,_ = readfile.read(msk_file)\n",
    "insar_velocities[msk == 0] = np.nan\n",
    "insar_velocities[insar_velocities == 0] = np.nan"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the mean linear velocity fit. The MintPy `view` module automatically reads the temporal coherence mask `maskTempCoh.h5` and applies that to mask out pixels with unreliable velocities (white)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scp_args = 'velocity.h5 velocity -v -20 20 --colormap RdBu_r --figtitle LOS_Velocity --unit mm/yr -m ' + msk_file\n",
    "view.main(scp_args.split())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Note :</b> \n",
    "Negative values indicates that target is moving away from the radar (i.e., Subsidence in case of vertical deformation).\n",
    "Positive values indicates that target is moving towards the radar (i.e., uplift in case of vertical deformation). \n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_co_gps'></a>\n",
    "## 4.2. Find Collocated GNSS Stations\n",
    "\n",
    "The project will have access to L2 position data for continuous GNSS stations in third-party networks such NSF’s Plate Boundary Observatory, the HVO network for Hawaii, GEONET-Japan, and GEONET-New Zealand, located in target regions for NISAR solid earth calval. Station data will be post-processed by one or more analysis centers, will be freely available, and will have latencies of several days to weeks, as is the case with positions currently produced by the NSF’s GAGE Facility and separately by the University of Nevada Reno. Networks will contain one or more areas of high-density station coverage (2~20 km nominal station spacing over 100 x 100 km or more) to support validation of L2 NISAR requirements at a wide range of length scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get analysis metadata from InSAR velocity file\n",
    "insar_metadata = readfile.read_attribute(vel_file)\n",
    "lat_step = float(insar_metadata['Y_STEP'])\n",
    "lon_step = float(insar_metadata['X_STEP'])\n",
    "(S,N,W,E) = ut.four_corners(insar_metadata)\n",
    "start_date = insar_metadata.get('START_DATE', None)\n",
    "end_date = insar_metadata.get('END_DATE', None)\n",
    "start_date_gnss = dt.strptime(start_date, \"%Y%m%d\")\n",
    "end_date_gnss = dt.strptime(end_date, \"%Y%m%d\")\n",
    "\n",
    "geom_file = os.path.join(mintpy_dir, 'inputs/geometryGeo.h5')\n",
    "inc_angle = readfile.read(geom_file, datasetName='incidenceAngle')[0]\n",
    "inc_angle = np.nanmean(inc_angle)\n",
    "az_angle = readfile.read(geom_file, datasetName='azimuthAngle')[0]\n",
    "az_angle = np.nanmean(az_angle)\n",
    "\n",
    "#Set GNSS Parameters\n",
    "gps_completeness_threshold = 0.9    #0.9  #percent of data timespan with valid GNSS epochs\n",
    "gps_residual_stdev_threshold = 10.  #0.03  #0.03  #max threshold standard deviation of residuals to linear GNSS fit\n",
    "\n",
    "# search for collocated GNSS stations\n",
    "site_names, site_lats, site_lons = gps.search_gps(SNWE=(S,N,W,E), start_date=start_date, end_date=end_date)\n",
    "site_names = [str(stn) for stn in site_names]\n",
    "print(\"Initial list of {} stations used in analysis:\".format(len(site_names)))\n",
    "print(site_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_gps_ts'></a>\n",
    "## 4.3. Get GNSS Position Time Series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get daily position solutions for GNSS stations\n",
    "use_stn = []  #stations to keep\n",
    "bad_stn = []  #stations to toss\n",
    "use_lats = [] \n",
    "use_lons = []\n",
    "\n",
    "for counter, stn in enumerate(site_names):\n",
    "    gps_obj = gps.GPS(site = stn, data_dir = os.path.join(mintpy_dir,'GPS'))\n",
    "    gps_obj.open(print_msg=False)\n",
    "    \n",
    "    # count number of dates in time range\n",
    "    dates = gps_obj.dates\n",
    "    range_days = (end_date_gnss - start_date_gnss).days\n",
    "    gnss_count = np.histogram(dates, bins=[start_date_gnss, end_date_gnss])\n",
    "    gnss_count = int(gnss_count[0])\n",
    "    \n",
    "    # for this quick screening check of data quality, we use the constant incidence and azimuth angles \n",
    "    # get standard deviation of residuals to linear fit\n",
    "    disp_los = ut.enu2los(gps_obj.dis_e, gps_obj.dis_n, gps_obj.dis_u, inc_angle, az_angle)\n",
    "    disp_detrended = signal.detrend(disp_los)\n",
    "    stn_stdv = np.std(disp_detrended)\n",
    "   \n",
    "    # select GNSS stations based on data completeness and scatter of residuals\n",
    "    disp_detrended = signal.detrend(disp_los)\n",
    "    if range_days * gps_completeness_threshold <= gnss_count:\n",
    "        if stn_stdv > gps_residual_stdev_threshold:\n",
    "            bad_stn.append(stn)\n",
    "        else:\n",
    "            use_stn.append(stn)\n",
    "            use_lats.append(site_lats[counter])\n",
    "            use_lons.append(site_lons[counter])\n",
    "    else:\n",
    "        bad_stn.append(stn)\n",
    "\n",
    "site_names = use_stn\n",
    "site_lats = use_lats\n",
    "site_lons = use_lons\n",
    "\n",
    "# [optional] manually remove additional stations\n",
    "gnss_to_remove=[]\n",
    "\n",
    "for i, gnss_site in enumerate(gnss_to_remove):\n",
    "    if gnss_site in site_names:\n",
    "        site_names.remove(gnss_site)\n",
    "    if gnss_site not in bad_stn:\n",
    "        bad_stn.append(gnss_site)\n",
    "\n",
    "print(\"\\nFinal list of {} stations used in analysis:\".format(len(site_names)))\n",
    "print(site_names)\n",
    "print(\"List of {} stations removed from analysis\".format(len(bad_stn)))\n",
    "print(bad_stn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_gps_los'></a>\n",
    "## 4.4. Project GNSS to LOS Velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gnss_velocities = gps.get_gps_los_obs(insar_metadata, \n",
    "                            'velocity', \n",
    "                            site_names, \n",
    "                            start_date=start_date,\n",
    "                            end_date=end_date,\n",
    "                            gps_comp='enu2los', \n",
    "                            redo=True)\n",
    "\n",
    "# scale site velocities from m/yr to mm/yr\n",
    "gnss_velocities *= 1000.\n",
    "\n",
    "print('\\n site   vel_los [mm/yr]')\n",
    "print(np.array([site_names, gnss_velocities]).T)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_gps_insar'></a>\n",
    "## 4.5. Re-Reference GNSS and InSAR LOS Velocities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reference GNSS stations to GNSS reference site\n",
    "ref_site_ind = site_names.index(sites[site]['gps_ref_site_name'])\n",
    "gnss_velocities = gnss_velocities - gnss_velocities[ref_site_ind]\n",
    "\n",
    "# reference InSAR to GNSS reference site\n",
    "ref_site_lat = float(site_lats[ref_site_ind])\n",
    "ref_site_lon = float(site_lons[ref_site_ind])\n",
    "ref_y, ref_x = ut.coordinate(insar_metadata).geo2radar(ref_site_lat, ref_site_lon)[:2]\n",
    "if not math.isnan(insar_velocities[ref_y, ref_x]):\n",
    "    insar_velocities = insar_velocities - insar_velocities[ref_y, ref_x]\n",
    "\n",
    "# plot GNSS stations on InSAR velocity field\n",
    "vmin = sites[site]['vmin']\n",
    "vmax = sites[site]['vmax']\n",
    "cmap = plt.get_cmap('RdBu_r')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "cax = ax.imshow(insar_velocities, cmap=cmap, vmin=vmin, vmax=vmax, interpolation='nearest', extent=(W, E, S, N))\n",
    "cbar = fig.colorbar(cax, ax=ax)\n",
    "cbar.set_label('LOS velocity [mm/year]')\n",
    "\n",
    "for lat, lon, obs in zip(site_lats, site_lons, gnss_velocities):\n",
    "    color = cmap((obs - vmin)/(vmax - vmin))\n",
    "    ax.scatter(lon, lat, color=color, s=8**2, edgecolors='k')\n",
    "for i, label in enumerate(site_names):\n",
    "     plt.annotate(label, (site_lons[i], site_lats[i]), color='black')\n",
    "\n",
    "out_fig = os.path.abspath('vel_insar_vs_gnss.png')\n",
    "fig.savefig(out_fig, bbox_inches='tight', transparent=True, dpi=300)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_nisar_validation'></a>\n",
    "# 5. NISAR Validation Approach 1: GNSS-InSAR Direct Comparison \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_make_vel'></a>\n",
    "## 5.1. Make Velocity Residuals at GNSS Locations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Set Parameters\n",
    "pixel_radius = 5   #number of InSAR pixels to average for comparison with GNSS\n",
    "\n",
    "#Create dictionary with the stations as the key and all their info as an array \n",
    "stn_dict = {}\n",
    "\n",
    "#Loop over GNSS station locations\n",
    "for i in range(len(site_names)): \n",
    "    # convert GNSS station lat/lon information to InSAR x/y grid\n",
    "    stn_lat = site_lats[i]\n",
    "    stn_lon = site_lons[i]\n",
    "    x_value = round((stn_lon - W)/lon_step)\n",
    "    y_value = round((stn_lat - N)/lat_step)\n",
    "    \n",
    "    # get velocities and residuals\n",
    "    gnss_site_vel = gnss_velocities[i]\n",
    "    #Caution: If you expand the radius parameter farther than the bounding grid it will break. \n",
    "    #To fix, remove the station in section 4 when the site_names list is filtered\n",
    "    vel_px_rad = insar_velocities[y_value-pixel_radius:y_value+1+pixel_radius, \n",
    "                     x_value-pixel_radius:x_value+1+pixel_radius]\n",
    "    insar_site_vel = np.median(vel_px_rad)\n",
    "    residual = gnss_site_vel - insar_site_vel\n",
    "\n",
    "    # populate data structure\n",
    "    values = [x_value, y_value, insar_site_vel, gnss_site_vel, residual, stn_lat, stn_lon]\n",
    "    stn = site_names[i]\n",
    "    stn_dict[stn] = values\n",
    "\n",
    "# extract data from structure\n",
    "res_list = []\n",
    "insar_site_vels = []\n",
    "gnss_site_vels = []\n",
    "lat_list = []\n",
    "lon_list = []\n",
    "for i in range(len(site_names)): \n",
    "    stn = site_names[i]\n",
    "    insar_site_vels.append(stn_dict[stn][2])\n",
    "    gnss_site_vels.append(stn_dict[stn][3])\n",
    "    res_list.append(stn_dict[stn][4])\n",
    "    lat_list.append(stn_dict[stn][5])\n",
    "    lon_list.append(stn_dict[stn][6])\n",
    "num_stn = len(site_names) \n",
    "print('Finish creating InSAR residuals at GNSS sites')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_make_velres'></a>\n",
    "## 5.2. Make Double-Differenced Velocity Residuals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_gps_sites = len(site_names)\n",
    "diff_res_list = []\n",
    "stn_dist_list = []\n",
    "\n",
    "# loop over stations\n",
    "for i in range(n_gps_sites-1):\n",
    "    stn1 = site_names[i]\n",
    "    for j in range(i + 1, n_gps_sites):\n",
    "        stn2 = site_names[j]\n",
    "\n",
    "        # calculate GNSS and InSAR velocity differences between stations\n",
    "        gps_vel_diff = stn_dict[stn1][3] - stn_dict[stn2][3]\n",
    "        insar_vel_diff = stn_dict[stn1][2] - stn_dict[stn2][2]\n",
    "\n",
    "        # calculate GNSS vs InSAR differences (double differences) between stations\n",
    "        diff_res = gps_vel_diff - insar_vel_diff\n",
    "        diff_res_list.append(diff_res)\n",
    "\n",
    "        # get distance (km) between stations using Haversine formula\n",
    "        # index 5 is lat, 6 is lon\n",
    "        stn_dist = haversine_distance(stn_dict[stn1][6], stn_dict[stn1][5], stn_dict[stn2][6], stn_dict[stn2][5])\n",
    "        stn_dist_list.append(stn_dist)\n",
    "\n",
    "# Write data for statistical tests\n",
    "gnss_site_dist = np.array(stn_dist_list)\n",
    "double_diff_rel_measure = np.array(np.abs(diff_res_list))\n",
    "ndx = np.argsort(gnss_site_dist)\n",
    "\n",
    "# Plot data to be used below\n",
    "fig, ax = plt.subplots(figsize=[11, 7])\n",
    "plt.scatter(gnss_site_dist, diff_res_list, label='V_gnss - V_InSAR for station pair')\n",
    "plt.axhline(secular_gnss_rqmt, color='r', linestyle='--', label='Secular rqmt')\n",
    "plt.axhline(-1*secular_gnss_rqmt, color='r', linestyle='--', label='Secular rqmt')\n",
    "plt.ylim(-10,10)\n",
    "plt.xlim(*gnss_dist_rqmt)\n",
    "plt.legend(loc='upper left')\n",
    "plt.title(f\"Double-Difference Residuals \\n Date range {start_date}-{end_date} \\n GNSS-InSAR velocities\")\n",
    "plt.xlabel(\"Distance (km)\")\n",
    "plt.ylabel(\"Double-Differenced Velocity Residual (mm/y)\")\n",
    "plt.show()\n",
    "\n",
    "out_fig = os.path.abspath('secular_insar-gnss_velocity_vs_distance.png')\n",
    "fig.savefig(out_fig, bbox_inches='tight', transparent=True, dpi=300)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Final result Method 1—Successful when 68% of points below requirements line\n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_valid_method1'></a>\n",
    "## 5.3. Secular Requirement Validation: Method 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Statistics\n",
    "n_bins = 10\n",
    "threshold = 0.683\n",
    "\n",
    "if findMax == 'true':\n",
    "    thresh_flag = 'false'\n",
    "else :\n",
    "    thresh_flag = 'true'\n",
    "\n",
    "tmp_secular_gnss_rqmt = deepcopy(secular_gnss_rqmt)\n",
    "sucess_flag = thresh_flag\n",
    "\n",
    "#  we assume that the distribution of residuals is Gaussian and \n",
    "#  that the threshold represents a 1-sigma limit within which \n",
    "#  we expect 68.3% of residuals to lie.\n",
    "\n",
    "# define bins and data columns, the final column is the ratio as a whole\n",
    "bins = np.linspace(*gnss_dist_rqmt, num=n_bins+1)\n",
    "n_all = np.empty((n_bins+1), dtype=int) # number of points in each bin\n",
    "n_pass = np.empty((n_bins+1), dtype=int) # number of points that pass criterion\n",
    "ratio = np.empty((n_bins+1), dtype=float) # ratio of points that pass criterion\n",
    "\n",
    "# populate bins\n",
    "inds = np.digitize(gnss_site_dist, bins)\n",
    "while sucess_flag == thresh_flag:\n",
    "    for i in range(n_bins):\n",
    "        # relative measurement\n",
    "        gnss_rem = double_diff_rel_measure[inds == i+1]\n",
    "        n_all[i] = np.count_nonzero(~np.isnan(gnss_rem))\n",
    "        n_pass[i] = np.count_nonzero(gnss_rem < tmp_secular_gnss_rqmt)\n",
    "        if n_all[i] == 0:\n",
    "            ratio[i] = 1.000  # assume pass if no data fall in bin\n",
    "        else:\n",
    "            ratio[i] = n_pass[i]/n_all[i]\n",
    "\n",
    "    # fill in last column\n",
    "    n_all[-1] = np.sum(n_all[0:-1])\n",
    "    n_pass[-1] = np.sum(n_pass[0:-1])\n",
    "    ratio[-1] = n_pass[-1]/n_all[-1]\n",
    "\n",
    "    # determine success or failure for each bin\n",
    "    success_or_fail = ratio > threshold  # boolean array\n",
    "    success_or_fail_str = np.array([['true' if x==True else 'false' for x in success_or_fail]])\n",
    "\n",
    "    # build pandas table\n",
    "    columns = []\n",
    "    for i in range(n_bins):\n",
    "        columns.append(f'{bins[i]:.2f}-{bins[i+1]:.2f}')\n",
    "    columns.append('total')\n",
    "\n",
    "    index = ['-'.join([start_date, end_date])]\n",
    "\n",
    "    # Display Results\n",
    "    n_all_pd = pd.DataFrame(n_all.reshape(1,n_bins+1),columns=columns,index=index)\n",
    "    n_pass_pd = pd.DataFrame(n_pass.reshape(1,n_bins+1),columns=columns,index=index)\n",
    "    ratio_pd = pd.DataFrame(ratio.reshape(1,n_bins+1),columns=columns,index=index)\n",
    "    success_or_fail_pd = pd.DataFrame(success_or_fail_str.reshape(1,n_bins+1),columns=columns,index=index)\n",
    "\n",
    "    #display(n_all_pd)  # Number of data points in each bin\n",
    "    #display(n_pass_pd) # Number of data points that lie below the curve\n",
    "\n",
    "    #Set new style for table\n",
    "    s = ratio_pd.style\n",
    "    s.set_table_styles([  # create internal CSS classes\n",
    "        {'selector': '.true', 'props': 'background-color: #e6ffe6;'},\n",
    "        {'selector': '.false', 'props': 'background-color: #ffe6e6;'},\n",
    "    ], overwrite=False)\n",
    "    #display(s.set_td_classes(success_or_fail_pd))  # Percentage of passing points:\n",
    "    #display(success_or_fail_pd)  # Explicit pass/fail table\n",
    "    sucess_flag = success_or_fail_pd.iloc[0]['total']\n",
    "\n",
    "    if findMax == 'true' :\n",
    "        tmp_secular_gnss_rqmt += 0.01\n",
    "    else :\n",
    "        tmp_secular_gnss_rqmt -= 0.01\n",
    "\n",
    "display(n_all_pd)  # Number of data points in each bin\n",
    "display(n_pass_pd) # Number of data points that lie below the curve\n",
    "display(s.set_td_classes(success_or_fail_pd))  # Percentage of passing points:\n",
    "display(success_or_fail_pd)  # Explicit pass/fail table\n",
    "\n",
    "print(tmp_secular_gnss_rqmt, success_or_fail_pd.iloc[0]['total'])\n",
    "# Overall pass/fail criterion\n",
    "if success_or_fail_pd.iloc[0]['total'] == 'true':\n",
    "    print(\"This velocity dataset passes the requirement.\")\n",
    "elif success_or_fail_pd.iloc[0]['total'] == 'false':\n",
    "    print(\"This velocity dataset does not pass the requirement.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Final result Method 1 table by distance bin—successful when greater than 0.683\n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='secular_nisar_validation2'></a>\n",
    "# 6. NISAR Validation Approach 2: InSAR-only Structure Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Validation approach 2, we use a time interval and area where we assume no deformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot velocity map\n",
    "scp_args = 'velocity.h5 velocity -v -20 20 --colormap RdBu_r --figtitle LOS_Velocity --unit mm/yr -m ' + msk_file\n",
    "view.main(scp_args.split())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='secular_array_mask'></a>\n",
    "## 6.1. Read Array and Mask Pixels with no Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the assumed non-earthquake displacement as the insar_displacment for statistics and convert to mm\n",
    "insar_velocities,_ = readfile.read(vel_file, datasetName = 'velocity')  #read velocity\n",
    "velStart = sites[site]['download_start_date']\n",
    "insar_velocities = insar_velocities * 1000.  # convert velocities from m to mm\n",
    "\n",
    "# set masked pixels to NaN\n",
    "msk,_ = readfile.read(msk_file)\n",
    "insar_velocities[msk == 0] = np.nan\n",
    "insar_velocities[insar_velocities == 0] = np.nan\n",
    "\n",
    "# display map of data after masking\n",
    "cmap = plt.get_cmap('RdBu_r')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "img1 = ax.imshow(insar_velocities, cmap=cmap, vmin=-20, vmax=20, interpolation='nearest', extent=(W, E, S, N))\n",
    "ax.set_title(\"Secular \\n Date \"+velStart)\n",
    "cbar1 = fig.colorbar(img1, ax=ax)\n",
    "cbar1.set_label('LOS velocity [mm/year]')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='secular_remove_trend'></a>\n",
    "## 6.2. Randomly Sample Pixels and Pair Them Up with Option to Remove Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_mode = 'profile'  # 'points' or 'profile'\n",
    "# note that the 'profile' method may take significantly longer\n",
    "\n",
    "# Collect samples using the specified method\n",
    "if sample_mode in ['points']:\n",
    "    X0,Y0 = load_geo(insar_metadata)\n",
    "    X0_2d,Y0_2d = np.meshgrid(X0,Y0)\n",
    "\n",
    "    insar_sample_dist, insar_rel_measure = samp_pair(X0_2d, Y0_2d, insar_velocities, num_samples=1000000)\n",
    "\n",
    "elif sample_mode in ['profile']:\n",
    "    # Sample grid setup\n",
    "    length, width = int(insar_metadata['LENGTH']), int(insar_metadata['WIDTH'])\n",
    "    X = np.linspace(W+lon_step, E-lon_step, width)  # longitudes\n",
    "    Y = np.linspace(N+lat_step, S-lat_step, length)  # latitudes\n",
    "    X_coords, Y_coords = np.meshgrid(X, Y)\n",
    "\n",
    "    # Draw random samples from map (without replacement)\n",
    "    num_samples = 20000\n",
    "    \n",
    "    # Retrieve profile samples\n",
    "    insar_sample_dist, insar_rel_measure = profile_samples(\\\n",
    "                    x=X_coords.reshape(-1,1),\n",
    "                    y=Y_coords.reshape(-1,1),\n",
    "                    data=insar_velocities,\n",
    "                    metadata=insar_metadata,\n",
    "                    len_rqmt=insar_dist_rqmt,\n",
    "                    num_samples=num_samples)\n",
    "\n",
    "print('Finished sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "img1 = ax.hist(insar_sample_dist, bins=100)\n",
    "ax.set_title(\"Histogram of distance \\n Secular Date {:s} - {:s}\".format(start_date, end_date))\n",
    "ax.set_xlabel(r'Distance ($km$)')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_xlim(*insar_dist_rqmt)\n",
    "    \n",
    "fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "img1 = ax.hist(insar_rel_measure, bins=100)\n",
    "ax.set_title(\"Histogram of Relative Measurement \\n Secular Date {:s} - {:s}\".format(start_date, end_date))\n",
    "ax.set_xlabel(r'Relative Measurement ($mm/year$)')\n",
    "ax.set_ylabel('Frequency')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='secular_M2ampvsdist2'></a>\n",
    "## 6.3. Amplitude vs. Distance of Relative Measurements (pair differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[18, 7.5])\n",
    "plt.axhline(secular_insar_rqmt, color='r', linestyle='--', label='Secular rqmt')\n",
    "ax.scatter(insar_sample_dist, insar_rel_measure, s=1, alpha=0.25, label='Relative velocity for pixel pair')\n",
    "ax.set_title(f\"Method 2: Relative Velocity Measurements between Pixel Pairs and Requirement Curve vs. Distance \\n {site} Secular Date range {start_date}-{end_date}\" )\n",
    "ax.set_ylabel(r'Relative Velocity Measurement ($mm/year$)')\n",
    "ax.set_xlabel('Distance (km)')\n",
    "ax.set_xlim(*insar_dist_rqmt)\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "out_fig = os.path.abspath('secular_insar-only_vs_distance_'+site+'_date'+velStart+'.png')\n",
    "fig.savefig(out_fig, bbox_inches='tight', transparent=True, dpi=300)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Final result Method 2—\n",
    "    68% of points below the requirements line is success\n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='secular_M2RelMeasTable'></a>\n",
    "## 6.4. Bin Sample Pairs by Distance Bin and Calculate Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Statistics\n",
    "n_bins = 10\n",
    "threshold = 0.683 \n",
    "\n",
    "if findMax == 'true':\n",
    "    thresh_flag = 'false'\n",
    "else :\n",
    "    thresh_flag = 'true'\n",
    "\n",
    "tmp_secular_insar_rqmt = deepcopy(secular_insar_rqmt)\n",
    "sucess_flag = thresh_flag\n",
    "\n",
    "#  we assume that the distribution of residuals is Gaussian and \n",
    "#  that the threshold represents a 1-sigma limit within which \n",
    "#  we expect 68.3% of residuals to lie.\n",
    "\n",
    "# define bins and data columns, the final column is the ratio as a whole\n",
    "bins = np.linspace(*insar_dist_rqmt, num=n_bins+1)\n",
    "n_all = np.empty((n_bins+1), dtype=int) # number of points in each bin\n",
    "n_pass = np.empty((n_bins+1), dtype=int) # number of points that pass criterion\n",
    "ratio = np.empty((n_bins+1), dtype=float) # ratio of points that pass criterion\n",
    "\n",
    "# populate bins\n",
    "inds = np.digitize(insar_sample_dist, bins)\n",
    "while sucess_flag == thresh_flag:\n",
    "    for i in range(n_bins):\n",
    "        # relative measurement\n",
    "        insar_rem = insar_rel_measure[inds == i+1]\n",
    "        n_all[i] = np.count_nonzero(~np.isnan(insar_rem))\n",
    "        n_pass[i] = np.count_nonzero(insar_rem < tmp_secular_insar_rqmt)\n",
    "        if n_all[i] == 0:\n",
    "            ratio[i] = 1.000  # assume pass if no data fall in bin\n",
    "        else:\n",
    "            ratio[i] = n_pass[i]/n_all[i]\n",
    "\n",
    "    # fill in last column\n",
    "    n_all[-1] = np.sum(n_all[0:-1])\n",
    "    n_pass[-1] = np.sum(n_pass[0:-1])\n",
    "    ratio[-1] = n_pass[-1]/n_all[-1]\n",
    "\n",
    "    # determine success or failure for each bin\n",
    "    success_or_fail = ratio > threshold  # boolean array\n",
    "    success_or_fail_str = np.array([['true' if x==True else 'false' for x in success_or_fail]])\n",
    "\n",
    "    # build pandas table\n",
    "    columns = []\n",
    "    for i in range(n_bins):\n",
    "        columns.append(f'{bins[i]:.2f}-{bins[i+1]:.2f}')\n",
    "    columns.append('total')\n",
    "\n",
    "    index = ['-'.join([start_date, end_date])]\n",
    "        \n",
    "    # Display Results\n",
    "    n_all_pd = pd.DataFrame(n_all.reshape(1,n_bins+1),columns=columns,index=index)\n",
    "    n_pass_pd = pd.DataFrame(n_pass.reshape(1,n_bins+1),columns=columns,index=index)\n",
    "    ratio_pd = pd.DataFrame(ratio.reshape(1,n_bins+1),columns=columns,index=index)\n",
    "    success_or_fail_pd = pd.DataFrame(success_or_fail_str.reshape(1,n_bins+1),columns=columns,index=index)\n",
    "\n",
    "    #display(n_all_pd)  # Number of data points in each bin\n",
    "    #display(n_pass_pd) # Number of data points that lie below the curve\n",
    "\n",
    "    #Set new style for table\n",
    "    s = ratio_pd.style\n",
    "    s.set_table_styles([  # create internal CSS classes\n",
    "        {'selector': '.true', 'props': 'background-color: #e6ffe6;'},\n",
    "        {'selector': '.false', 'props': 'background-color: #ffe6e6;'},\n",
    "    ], overwrite=False)\n",
    "    #display(s.set_td_classes(success_or_fail_pd))  # Percentage of passing points:\n",
    "    #display(success_or_fail_pd)  # Explicit pass/fail table\n",
    "    sucess_flag = success_or_fail_pd.iloc[0]['total']\n",
    "    if findMax == 'true' :\n",
    "        tmp_secular_insar_rqmt += 0.01\n",
    "    else :\n",
    "        tmp_secular_insar_rqmt -= 0.01\n",
    "\n",
    "display(n_all_pd)  # Number of data points in each bin\n",
    "display(n_pass_pd) # Number of data points that lie below the curve\n",
    "display(s.set_td_classes(success_or_fail_pd))  # Percentage of passing points:\n",
    "display(success_or_fail_pd)  # Explicit pass/fail table\n",
    "\n",
    "print(tmp_secular_insar_rqmt, success_or_fail_pd.iloc[0]['total'])\n",
    "# Overall pass/fail criterion\n",
    "if success_or_fail_pd.iloc[0]['total'] == 'true':\n",
    "    print(\"This velocity dataset passes the requirement.\")\n",
    "elif success_or_fail_pd.iloc[0]['total'] == 'false':\n",
    "    print(\"This velocity dataset does not pass the requirement.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Final result Method 2 table of distance bins—\n",
    "    68% (0.683) of points below the requirements line is success\n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_appendix1'></a>\n",
    "# Appendix: Supplementary Comparisons and Plots"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_compare_raw'></a>\n",
    "## A.1. Compare Raw Velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vmin, vmax = -25, 25\n",
    "plt.figure(figsize=(11,7))\n",
    "plt.hist(insar_site_vels, range=[vmin, vmax], bins=50, color=\"green\", edgecolor='grey', label='V_InSAR')\n",
    "plt.hist(gnss_site_vels, range=[vmin, vmax], bins=50, color=\"orange\", edgecolor='grey', label='V_gnss', alpha=0.5)\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(f\"Velocities \\n Date range {start_date}-{end_date} \\n Reference stn: {sites[site]['gps_ref_site_name']} \\n Number of stations used: {num_stn}\")\n",
    "plt.xlabel('LOS Velocity (mm/year)')\n",
    "plt.ylabel('N Stations')\n",
    "plt.ylim(0,20)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_plot_vel'></a>\n",
    "## A.2. Plot Velocity Residuals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vmin, vmax = -10, 10\n",
    "plt.figure(figsize=(11,7))\n",
    "plt.hist(res_list, bins = 40, range=[vmin,vmax], edgecolor='grey', color=\"darkblue\", linewidth=1, label='V_gnss - V_InSAR (area average)')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(f\"Residuals \\n Date range {start_date}-{end_date} \\n Reference stn: {sites[site]['gps_ref_site_name']} \\n Number of stations used: {num_stn}\")\n",
    "plt.xlabel('Velocity Residual (mm/year)')\n",
    "plt.ylabel('N Stations')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_plot_velres'></a>\n",
    "## A.3. Plot Double Difference Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11,7))\n",
    "plt.hist(diff_res_list, range = [vmin, vmax],bins = 40, color = \"darkblue\",edgecolor='grey',label='V_gnss_(s1-s2) - V_InSAR_(s1-s2)')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(f\"Difference Residualts \\n Date range {start_date}-{end_date} \\n Reference stn: {sites[site]['gps_ref_site_name']} \\n Number of stations used: {num_stn}\")\n",
    "plt.xlabel('Double Differenced Velocity Residual (mm/year)')\n",
    "plt.ylabel('N Stations')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_appendix_gps'></a>\n",
    "## A.4. GNSS Timeseries Plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# grab the time-series file used for time function estimation given the template setup\n",
    "template = readfile.read_template(os.path.join(mintpy_dir, 'smallbaselineApp.cfg'))\n",
    "template = ut.check_template_auto_value(template)\n",
    "insar_ts_file = TimeSeriesAnalysis.get_timeseries_filename(template, mintpy_dir)['velocity']['input']\n",
    "\n",
    "# read the time-series file\n",
    "insar_ts, atr = readfile.read(insar_ts_file, datasetName='timeseries')\n",
    "mask = readfile.read(os.path.join(mintpy_dir, 'maskTempCoh.h5'))[0]\n",
    "print(f'reading timeseries from file: {insar_ts_file}')\n",
    "\n",
    "# Get date list\n",
    "date_list = timeseries(insar_ts_file).get_date_list()\n",
    "num_date = len(date_list)\n",
    "date0, date1 = date_list[0], date_list[-1]\n",
    "insar_dates = ptime.date_list2vector(date_list)[0]\n",
    "\n",
    "# spatial reference\n",
    "coord = ut.coordinate(atr)\n",
    "ref_site = sites[site]['gps_ref_site_name']\n",
    "ref_gnss_obj = gps.GPS(site=ref_site, data_dir=mintpy_dir/'GPS')\n",
    "ref_lat, ref_lon = ref_gnss_obj.get_stat_lat_lon()\n",
    "ref_y, ref_x = coord.geo2radar(ref_lat, ref_lon)[:2]\n",
    "if not mask[ref_y, ref_x]:\n",
    "    raise ValueError(f'Given reference GNSS site ({ref_site}) is in mask-out unrelible region in InSAR! Change to a different site.')\n",
    "ref_insar_dis = insar_ts[:, ref_y, ref_x]\n",
    "\n",
    "# Plot displacements and velocity timeseries at GNSS station locations\n",
    "num_site = len(site_names)\n",
    "prog_bar = ptime.progressBar(maxValue=num_site)\n",
    "for i, site_name in enumerate(site_names):\n",
    "    prog_bar.update(i+1, suffix=f'{site_name} {i+1}/{num_site}')\n",
    "\n",
    "    ## read data\n",
    "    # read GNSS\n",
    "    gnss_obj = gps.GPS(site=site_name, data_dir=mintpy_dir/'GPS')\n",
    "    gnss_dates, gnss_dis, _, gnss_lalo = gnss_obj.read_gps_los_displacement(atr, start_date=date0, end_date=date1, ref_site=ref_site)[:4]\n",
    "    # shift GNSS to zero-mean in time [for plotting purpose]\n",
    "    gnss_dis -= np.nanmedian(gnss_dis)\n",
    "\n",
    "    # read InSAR\n",
    "    y, x = coord.geo2radar(gnss_lalo[0], gnss_lalo[1])[:2]\n",
    "    insar_dis = insar_ts[:, y, x] - ref_insar_dis\n",
    "    # apply a constant shift in time to fit InSAR to GNSS\n",
    "    comm_dates = sorted(list(set(gnss_dates) & set(insar_dates)))\n",
    "    if comm_dates:\n",
    "        insar_flag = [x in comm_dates for x in insar_dates]\n",
    "        gnss_flag = [x in comm_dates for x in gnss_dates]\n",
    "        insar_dis -= np.nanmedian(insar_dis[insar_flag] - gnss_dis[gnss_flag])\n",
    "\n",
    "    ## plot figure\n",
    "    if gnss_dis.size > 0 and np.any(~np.isnan(insar_dis)):\n",
    "        fig, ax = plt.subplots(figsize=(12, 3))\n",
    "        ax.axhline(color='grey',linestyle='dashed', linewidth=2)\n",
    "        ax.scatter(gnss_dates, gnss_dis*100, s=2**2, label=\"GNSS Daily Positions\")\n",
    "        ax.scatter(insar_dates, insar_dis*100, label=\"InSAR Positions\")\n",
    "        # axis format\n",
    "        ax.set_title(f\"Station Name: {site_name}\") \n",
    "        ax.set_ylabel('LOS displacement [cm]')\n",
    "        ax.legend()\n",
    "prog_bar.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
