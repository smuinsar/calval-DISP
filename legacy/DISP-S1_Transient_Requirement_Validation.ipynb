{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Workflow to Validate NISAR L2 Transient Displacement Requirements\n",
    "\n",
    "**Original code authored by:** NISAR Science Team Members and Affiliates  \n",
    "\n",
    "*May 13, 2022*\n",
    "\n",
    "*NISAR Solid Earth Team*\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "Both the initial setup (<b>Prep A</b> section) and download of the data (<b>Prep B</b> section) should be run at the start of the notebook. And all subsequent sections NEED to be run in order.\n",
    "</div>\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define CalVal Site "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters for papermill\n",
    "\n",
    "### Choose a site from the 'sites' dictionary found 2 cells down\n",
    "## If your CentralValleyD144 area is not defined, add a new dictionary entry as appropriate and provide a unique site keyname\n",
    "site = 'CentralValleyD144'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# injected parameters\n",
    "\n",
    "### Choose a site from the 'sites' dictionary found 2 cells down\n",
    "## If your study area is not defined, add a new dictionary entry as appropriate and provide a unique site keyname\n",
    "site='CentralValleyD144'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify GNSS source for validation\n",
    "from mintpy.objects import gnss\n",
    "gnss_source = 'UNR'\n",
    "print(f'Searching for all GNSS stations from source: {gnss_source}')\n",
    "print(f'May use any of the following supported sources: {gnss.GNSS_SOURCES}')\n",
    "GNSS = gnss.get_gnss_class(gnss_source)\n",
    "\n",
    "# Set mask file\n",
    "maskFile = 'maskConnComp.h5' # maskSpatialCoh.h5 maskTempCoh.h5 maskConnComp.h5 waterMask.h5 (maskConnComp.h5 is very conservative)\n",
    "\n",
    "# Define spatial coherence threshold (necessary to reject poor quality, long temporal baseline pairs)\n",
    "minCoherence_parm = '0.4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Table of Contents:\n",
    "<a id='secular_TOC'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "[**Prep A. Environment Setup**](#transient_prep_a)\n",
    "\n",
    "[**Prep B. Data Staging**](#transient_prep_b)\n",
    "\n",
    "[**1. Generate Interferogram Stack**](#transient_gen_ifg)\n",
    "- [1.1.  Crop Interferograms](#transient_crop_ifg)\n",
    "\n",
    "[**2. Optional Corrections**](#transient_opt_correction)\n",
    "- [2.1. Solid Earth Tides Correction](#transient_solid_earth)\n",
    "- [2.2. Tropospheric Delay Correction](#transient_tropo_corr)\n",
    "- [2.3. Topographic Residual Correction ](#transient_tropo_res_corr)\n",
    "\n",
    "[**3. Make GNSS LOS Measurements**](#transient_gnss_los)\n",
    "- [3.1. Find Collocated GNSS Stations](#transient_co_gnss)  \n",
    "- [3.2. Make GNSS LOS Measurements](#transient_gnss_los2) \n",
    "- [3.3. Make GNSS and InSAR Relative Displacements](#transient_gnss_insar)\n",
    "\n",
    "[**4. NISAR Validation Approach 1: GNSS-InSAR Direct Comparison**](#transient_validation1)\n",
    "- [4.1. Pair up GNSS stations and make measurement residuals](#transient_pair1)\n",
    "- [4.2. Validate the requirement based on binned measurement residuals](#transient_bin1)\n",
    "- [4.3. Result visualization](#transient_result1)\n",
    "- [4.3. Conclusion](#transient_conclusion1)\n",
    "\n",
    "[**5. NISAR Validation Approach 2: Noise Level Validation**](#transient_validation2)\n",
    "- [5.1. Randomly sample pixels and pair them up](#transient_pair2)\n",
    "- [5.2. Validate the requirement based on binned measurement residuals](#transient_bin2)\n",
    "- [5.3. Result visualization](#transient_result2)\n",
    "- [5.3. Conclusion](#transient_conclusion2)\n",
    "\n",
    "[**6. Appendix: GNSS Position Plots**](#transient_appendix)\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='transient_prep_a'></a>\n",
    "## Prep A. Environment Setup\n",
    "Setup your environment for processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "import concurrent.futures\n",
    "import copy\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyproj\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from mintpy.objects import gnss\n",
    "from mintpy.utils import readfile, utils as ut, utils0 as ut0, writefile\n",
    "\n",
    "from solid_utils.sampling import load_geo, load_geo_utm, samp_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "################# Set Directories ##########################################\n",
    "print('\\nCurrent directory:',os.getcwd())\n",
    "\n",
    "if 'work_dir' not in locals():\n",
    "    work_dir = Path.cwd()\n",
    "\n",
    "print(\"Work directory:\", work_dir)\n",
    "work_dir.mkdir(parents=True, exist_ok=True)\n",
    "# Change to Workdir   \n",
    "os.chdir(work_dir)\n",
    "\n",
    "mintpy_dir = work_dir/'MintPy' \n",
    "mintpy_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(\"   MintPy  dir:\", mintpy_dir)\n",
    "############################################################################\n",
    "### List of CalVal Sites:\n",
    "'''\n",
    "Set NISAR calval sites:\n",
    "    CentralValleyD144  : Central Valley\n",
    "    OklahomaA107       : Oklahoma\n",
    "    PuertoRicoD98      : Puerto Rico (Earthquake M6.4 on 20200107) - Descending track \n",
    "    PuertoRicoA135     : Puerto Rico (Earthquake M6.4 on 20200107 & large aftershock on 20200703) - Ascending track\n",
    "    RidgecrestD71      : Ridgecrest  (Earthquake M7.2 on 20190705) - Descending track\n",
    "    RidgecrestA64      : Ridgecrest  (Earthquake M7.2 on 20190705) - Ascending track\n",
    "\n",
    "ARIA & MintPy parameters:\n",
    "    calval_location : name\n",
    "    download_region : download box in S,N,W,E format\n",
    "    analysis_region : analysis box in S,N,W,E format (must be within download_region)\n",
    "    download_start_date : download start date as YYYMMDD  \n",
    "    download_end_date   : download end date as YYYMMDD\n",
    "    earthquakeDate :  arbitrary date for testing with the central_valley dataset\n",
    "    sentinel_track : sentinel track to download\n",
    "    gps_ref_site_name : Name of the GPS site for InSAR re-referencing\n",
    "    tempBaseMax' : maximum number of days, 'don't use interferograms longer than this value \n",
    "    ifgExcludeList : default is not to exclude any interferograms\n",
    "    maskWater' :  interior locations don't need to mask water\n",
    "'''\n",
    "sites = {\n",
    "    ##########  CENTRAL VALLEY ##############\n",
    "    'CentralValleyD144' : {'calval_location' : 'Central_Valley',\n",
    "            'download_region' : '\"36.18 36.26 -119.91 -119.77\"', # download box in S,N,W,E format\n",
    "            'analysis_region' : '\"35.77 36.75 -120.61 -118.06\"', # analysis box in S,N,W,E format (must be within download_region)\n",
    "            'download_start_date' : '20180101',\n",
    "            'download_end_date' : '20190101',\n",
    "            'earthquakeDate' : '20180412',                       # arbitrary date for testing with the central_valley dataset\n",
    "            'sentinel_track' : '144',\n",
    "            'gps_ref_site_name' : 'LAND',\n",
    "            'tempBaseMax' : 'auto',\n",
    "            'ifgExcludeList' : 'auto',\n",
    "            'maskWater' : False},                       # reference site for this area\n",
    "    ##########  HAWAII ##############\n",
    "    'HawaiiA124' : {'calval_location' : 'Hawaii',\n",
    "            'download_region' : '\"19.22 19.70 -155.73 -155.33\"', # download box in S,N,W,E format\n",
    "            'analysis_region' : '\"19.22 19.70 -155.73 -155.33\"', # analysis box in S,N,W,E format (must be within download_region)\n",
    "            'download_start_date' : '20180101',\n",
    "            'download_end_date' : '20190101',\n",
    "            'earthquakeDate' : '20180412',                       # arbitrary date for testing with the central_valley dataset\n",
    "            'sentinel_track' : '124',\n",
    "            'gps_ref_site_name' : 'YEEP',\n",
    "            'tempBaseMax' : 'auto',\n",
    "            'ifgExcludeList' : 'auto',\n",
    "            'maskWater' : False},                       # reference site for this area\n",
    "    ##########  DISP golden output Central Valley ##############\n",
    "    'DISP_golden_output_CV' : {'calval_location' : 'DISP_golden_output_CV',\n",
    "            'download_region' : '\"38.72314 39.31014 -121.92130 -121.40724\"', # download box in S,N,W,E format\n",
    "            'analysis_region' : '\"38.72314 39.31014 -121.92130 -121.40724\"', # analysis box in S,N,W,E format (must be within download_region)\n",
    "            'download_start_date' : '20221101',\n",
    "            'download_end_date' : '20230801',\n",
    "            'earthquakeDate' : '20180412',                       # arbitrary date for testing with the central_valley dataset\n",
    "            'sentinel_track' : '124',\n",
    "            'gps_ref_site_name' : 'SUTB',\n",
    "            'tempBaseMax' : 'auto',\n",
    "            'ifgExcludeList' : 'auto',\n",
    "            'maskWater' : False},                       # reference site for this area\n",
    "    ##########  CENTRAL VALLEY ##############\n",
    "    'CentralValleyD144_full' : {'calval_location' : 'CentralValleyD144_full',\n",
    "            'download_region' : '\"36.18 36.26 -119.91 -119.77\"', # download box in S,N,W,E format\n",
    "            'analysis_region' : '\"35.77 36.75 -120.61 -118.06\"', # analysis box in S,N,W,E format (must be within download_region)\n",
    "            'download_start_date' : '20180101',\n",
    "            'download_end_date' : '20200101',\n",
    "            'earthquakeDate' : '20180412',                       # arbitrary date for testing with the central_valley dataset\n",
    "            'sentinel_track' : '144',\n",
    "            'gps_ref_site_name' : 'LAND',\n",
    "            'tempBaseMax' : 'auto',\n",
    "            'ifgExcludeList' : 'auto',\n",
    "            'maskWater' : False},                       # reference site for this area\n",
    "    ##########  OKLAHOMA ##############\n",
    "    'OklahomaA107' : {'calval_location' : 'Oklahoma',\n",
    "            'download_region' : '\"31.7 37.4 -103.3 -93.5\"',      # download box in S,N,W,E format\n",
    "            'analysis_region' : '\"35.25 36.5 -100.5 -98.5\"',     # analysis box in S,N,W,E format (must be within download_region)\n",
    "            'download_start_date' : '20210101',\n",
    "            'download_end_date' : '20210801',\n",
    "            'earthquakeDate' : '20210328',                       # arbitrary date for testing with the Oklahoma dataset\n",
    "            'sentinel_track' : '107',\n",
    "            'gps_ref_site_name' : 'OKCL',\n",
    "            'tempBaseMax' : 'auto',\n",
    "            'ifgExcludeList' : 'auto',\n",
    "            'maskWater' : False},\n",
    "    ##########  PUERTO RICO ##############\n",
    "    'PuertoRicoD98' : {'calval_location' : 'PuertoRicoDesc',\n",
    "            'download_region' : '\"17.5 18.9 -67.5 -66.0\"',       # download box in S,N,W,E format\n",
    "            'analysis_region' : '\"17.9 18.5 -67.3 -66.2\"',       # analysis box in S,N,W,E format (must be within download_region)\n",
    "            'download_start_date' : '20190701',\n",
    "            'download_end_date' : '20200930',\n",
    "            'earthquakeDate' : '20200107',                       # date of M6.4 quake\n",
    "            'sentinel_track' : '98',                             # descending track\n",
    "            'gps_ref_site_name' : 'PRLT',\n",
    "            'tempBaseMax' : 24,                                  # don't use interferograms longer than 24 days\n",
    "            'ifgExcludeList' : 'auto', \n",
    "            'maskWater' : True},                                 # need to mask ocean around Puerto Rico island\n",
    "    'PuertoRicoA135' : {'calval_location' : 'PuertoRicoAsc',\n",
    "             'download_region' : '\"17.5 18.9 -67.5 -66.0\"',      # download box in S,N,W,E format\n",
    "             'analysis_region' : '\"17.9 18.5 -67.3 -66.2\"',      # analysis box in S,N,W,E format (must be within download_region)\n",
    "             'download_start_date' : '20190701',\n",
    "             'download_end_date' : '20200930',\n",
    "             'earthquakeDate' : '20200107',                      # date of M6.4 quake\n",
    "             'earthquakeDate2' : '20200703',                     # date of large aftershock\n",
    "             'sentinel_track' : '135',                           # ascending track\n",
    "             'gps_ref_site_name' : 'PRLT',\n",
    "             'tempBaseMax' : 24,                                 # don't use interferograms longer than 24 days\n",
    "             'ifgExcludeList' : 'auto',\n",
    "             'maskWater' : True},                                # need to mask ocean around Puerto Rico island\n",
    "    ##########  RIDGECREST ##############\n",
    "    'RidgecrestD71': {'calval_location' : 'RidgecrestD71',\n",
    "                      'download_region' : '\"34.5 37.5 -119.0 -116.0\"', # download box in S,N,W,E format\n",
    "                      'analysis_region' : '\"34.7 37.2 -118.9 -116.1\"', # analysis box in S,N,W,E format (must be within download_region)\n",
    "                      'download_start_date' : '20190601',\n",
    "                      'download_end_date' : '20190831',\n",
    "                      'earthquakeDate' : '20190705',                   # M7.2 quake date at Ridgecrest\n",
    "                      'sentinel_track' : '71',\n",
    "                      'gps_ref_site_name' : 'ISLK',\n",
    "                      'tempBaseMax' : 'auto',\n",
    "                      'ifgExcludeList' : 'auto',\n",
    "                      'maskWater' : False},\n",
    "    'RidgecrestA64': {'calval_location' : 'Ridgecrest',\n",
    "                      'download_region' : '\"34.5 37.5 -119.0 -116.0\"', # download box in S,N,W,E format\n",
    "                      'analysis_region' : '\"34.7 37.2 -118.9 -116.1\"', # analysis box in S,N,W,E format (must be within download_region)\n",
    "                      'download_start_date' : '20190101',\n",
    "                      'download_end_date' : '20191231',\n",
    "                      'earthquakeDate' : '20190705',                   # M7.2 quake date at Ridgecrest\n",
    "                      'sentinel_track' : '64',\n",
    "                      'gps_ref_site_name' : 'ISLK',\n",
    "                      'tempBaseMax' : 'auto',\n",
    "                      'ifgExcludeList' : '[50,121,123,124,125,126]',   # list of bad ifgs to exclude from time-series analysis\n",
    "                      'maskWater' : False},\n",
    "\n",
    "    ##########  HOUSTON ##############\n",
    "    'HoustonD143' : {'calval_location' : 'smallbaselineApp',\n",
    "            'download_region' : '\"29.85 30.25 -95.6 -95.05\"', # download box in S,N,W,E format\n",
    "            'analysis_region' : '\"29.85 30.25 -95.6 -95.05\"', # analysis box in S,N,W,E format (must be within download_region)\n",
    "            'reference_lalo' : '29.692 -95.635',\n",
    "            'download_start_date' : '20211001',\n",
    "            'download_end_date' : '20220331',\n",
    "            'flight_dir' : 'des',\n",
    "            'sentinel_track' : '143',\n",
    "            'gps_ref_site_name' : 'ALEF', # reference site for this area \n",
    "            'tempBaseMax' : 'auto',\n",
    "            'maskWater' : False,\n",
    "            }\n",
    "}\n",
    "\n",
    "print('\\nSelected site: {}'.format(site))\n",
    "for key, value in sites[site].items():\n",
    "    print('   '+ key, ' : ', value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ifgs_file = mintpy_dir/'inputs/ifgramStack.h5'\n",
    "geom_file = str(mintpy_dir/'inputs/geometryGeo.h5')\n",
    "# check for existence of geometry file, for which the location may vary\n",
    "if not os.path.exists(geom_file):\n",
    "    alt_geom_path = str(mintpy_dir/'geometryGeo.h5')\n",
    "    if not os.path.exists(alt_geom_path):\n",
    "        raise Exception(f'Valid geometry file not found here ({geom_file}) nor here ({alt_geom_path})')\n",
    "    else:\n",
    "        geom_file = alt_geom_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** If the interferogram has a resolution lower than 100 m, we need multi-look the interferogram phase values before calculating the empirical semivarigram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the date of interferograms into Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ifgs_date = readfile.read(ifgs_file,datasetName='date')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ifgs_date = []\n",
    "download_start_date = sites[site]['download_start_date']\n",
    "download_end_date = sites[site]['download_end_date']\n",
    "download_start_date = dt.strptime(download_start_date, \"%Y%m%d\")\n",
    "download_end_date = dt.strptime(download_end_date, \"%Y%m%d\")\n",
    "for i in range(ifgs_date.shape[0]):\n",
    "    start_date = ifgs_date[i,0].decode()\n",
    "    end_date = ifgs_date[i,1].decode()\n",
    "    start_date = dt.strptime(start_date, \"%Y%m%d\")\n",
    "    end_date = dt.strptime(end_date, \"%Y%m%d\")\n",
    "    if download_start_date <= start_date and download_end_date >= end_date:\n",
    "        _ifgs_date.append([start_date,end_date])\n",
    "\n",
    "ifgs_date = np.array(_ifgs_date,dtype=dt)\n",
    "del _ifgs_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove interferograms with time interval other than 12 days:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_row_index = []\n",
    "for i in range(ifgs_date.shape[0]):\n",
    "\n",
    "    if i == 0:\n",
    "        time_interval = (ifgs_date[i,1]-ifgs_date[i,0]).days\n",
    "    else:\n",
    "        # if ifgs_date[i,0] the first date of all dataset\n",
    "        time_interval = (ifgs_date[i,1]-ifgs_date[i-1,1]).days\n",
    "\n",
    "    if time_interval > 12:\n",
    "        del_row_index.append(i)\n",
    "while i<ifgs_date.shape[0]-1:\n",
    "    if ifgs_date[i,1]==ifgs_date[i+1,0]:\n",
    "        del_row_index.append(i+1)\n",
    "        i = i+2\n",
    "    else:\n",
    "        i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ifgs_date = np.delete(ifgs_date,del_row_index,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify independent interferograms (i.e., selected inteferograms do NOT share common dates):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_row_index = []\n",
    "i = 0\n",
    "while i<ifgs_date.shape[0]-1:\n",
    "    if ifgs_date[i,1]==ifgs_date[i+1,0]:\n",
    "        del_row_index.append(i+1)\n",
    "        i = i+2\n",
    "    else:\n",
    "        i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ifgs_date = np.delete(ifgs_date,del_row_index,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the phase and coherence of selected interferograms, geometrical datasets, and attribution of them are loaded into numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unwrapPhaseName = ['unwrapPhase-'+i[0].strftime('%Y%m%d')+'_'+i[1].strftime('%Y%m%d') for i in ifgs_date]\n",
    "coherenceName = ['coherence-'+i[0].strftime('%Y%m%d')+'_'+i[1].strftime('%Y%m%d') for i in ifgs_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get shape of array\n",
    "ifgs_unw, atr = readfile.read(ifgs_file,datasetName=unwrapPhaseName[0])\n",
    "arr_shape = (len(unwrapPhaseName), ifgs_unw.shape[0], ifgs_unw.shape[1])\n",
    "del ifgs_unw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read unw phase\n",
    "insar_displacement = np.zeros(arr_shape, dtype=np.float32)\n",
    "for i in range(len(unwrapPhaseName)):\n",
    "    ifgs_unw, atr = readfile.read(ifgs_file,datasetName=unwrapPhaseName[i])\n",
    "    ifgs_unw = -ifgs_unw*float(atr['WAVELENGTH'])/(4*np.pi)*1000 # unit in mm\n",
    "    insar_displacement[i] = ifgs_unw\n",
    "    del ifgs_unw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the coherence array for large stacks can lead to out-of-memory errors\n",
    "# commenting out\n",
    "# Read coherence\n",
    "#insar_coherence = np.zeros(arr_shape, dtype=np.float32)\n",
    "#for i in range(len(coherenceName)):\n",
    "#    ifgs_coh, _ = readfile.read(ifgs_file,datasetName=coherenceName[i])\n",
    "#    insar_coherence[i] = ifgs_coh\n",
    "#    del ifgs_coh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change default missing phase values in interferograms from 0.0 to `np.nan`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insar_displacement[insar_displacement==0.0] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### *Experimental for applying nonlinear-displacement mask*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = readfile.read(os.path.join(mintpy_dir, maskFile))[0]\n",
    "mask_flag = False\n",
    "\n",
    "n_dates, row, col = insar_displacement.shape\n",
    "\n",
    "# set number of plots for debugging mask\n",
    "num_plot = 9\n",
    "if len(ifgs_date) < 9:\n",
    "    num_plot = len(ifgs_date)\n",
    "\n",
    "if mask_flag:\n",
    "    for ii in range(ifgs_date):\n",
    "        insar_displacement[ii,~mask] = np.nan  # applying mask\n",
    "\n",
    "    # checking if masking worked\n",
    "    random_inds = np.random.choice(n_dates, num_plot, replace=False)\n",
    "    random_inds = sorted(random_inds)\n",
    "\n",
    "    fig, ax = plt.subplots(num_plot//3, 3,figsize=(10,10))\n",
    "    fig.subplots_adjust(hspace = .001, wspace=.001)\n",
    "\n",
    "    axs = ax.ravel()\n",
    "\n",
    "    for i in range(num_plot):\n",
    "        axs[i].imshow(insar_displacement[random_inds[i],:,:], cmap='jet',vmin=-50,vmax=50)     # unit: mm\n",
    "        axs[i].axis('off')\n",
    "        axs[i].set_title(ifgs_date[random_inds[i],0].strftime('%Y%m%d')+'-'+ifgs_date[random_inds[i],1].strftime('%Y%m%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "window_size = 11  # Size of the rolling window\n",
    "\n",
    "variability_indices = np.zeros((row, col, n_dates - window_size + 1), dtype=np.float32)\n",
    "variability_indices = variability_indices.reshape(-1,n_dates - window_size + 1)     # converting 3D to 2D\n",
    "\n",
    "insar_displacement_masked = np.transpose(insar_displacement, (1,2,0))    # reshaping an array to have number of dates to last axis\n",
    "insar_displacement_masked = insar_displacement_masked.reshape(-1,n_dates)       # converting 3D to 2D\n",
    "\n",
    "nline = row * col\n",
    "\n",
    "mask_var_score_file = os.path.join(mintpy_dir, 'mask_var_score.h5')\n",
    "\n",
    "if not os.path.exists(mask_var_score_file):\n",
    "\n",
    "    nprocs = 100      # number of multi-core processors\n",
    "    npatch = 100      # number of patches in each row and col\n",
    "\n",
    "    step_line = nline // npatch\n",
    "\n",
    "    list_line = list(range(0,nline,step_line))\n",
    "    ncol = len(list_line)\n",
    "\n",
    "    params = []\n",
    "\n",
    "    for ind_line, start_line in enumerate(list_line):\n",
    "        if ind_line == (ncol - 1):\n",
    "            end_line = nline\n",
    "        else:\n",
    "            end_line = list_line[ind_line + 1]\n",
    "\n",
    "        for depth in range(n_dates - window_size + 1):\n",
    "            params.append((start_line, end_line, depth, window_size))\n",
    "\n",
    "    print('number of multi-processors: ', len(params))\n",
    "\n",
    "    # Define the function to be executed in parallel\n",
    "    def calculate_window_std(start_line: int, end_line: int, depth: int, window_size: int) -> Tuple[int, int, int, np.ndarray]:\n",
    "        _st = time.time()\n",
    "        print(f'Processing: {start_line}, {end_line}, {depth}\\n')\n",
    "        window_data = insar_displacement_masked[start_line:end_line, depth:depth+window_size] / 1000 # convert back to m\n",
    "        rolling_mean = pd.DataFrame(window_data).rolling(window=window_size, min_periods=1, axis=1).mean().values\n",
    "        window_std = np.std(rolling_mean, axis=-1)\n",
    "        _end = time.time()\n",
    "        _time_taken = np.round((_end - _st)/60.,2)\n",
    "        print(f'{_time_taken} min taken for patch')\n",
    "        return start_line, end_line, depth, window_std\n",
    "\n",
    "    st_time = time.time()\n",
    "\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=nprocs) as executor:\n",
    "        for result in executor.map(calculate_window_std, *zip(*params)):\n",
    "            start_line, end_line, depth, window_std = result\n",
    "            variability_indices[start_line:end_line, depth] = window_std\n",
    "            print(f'Finished: {start_line}, {end_line}, {depth}\\n')\n",
    "\n",
    "    variability_indices = variability_indices.reshape(row, col, n_dates - window_size + 1)  # converting 2D to 3D\n",
    "    \n",
    "    end_time = time.time()\n",
    "    time_taken = np.round((end_time - st_time)/60.,2)\n",
    "    print(f'{time_taken} min taken for multi-core non-linear masking processing')\n",
    "\n",
    "    sigma = np.std(variability_indices,axis=-1)\n",
    "    variability_scores = np.sum(variability_indices < sigma[..., np.newaxis], axis=2) / (n_dates - window_size + 1)     # ratio\n",
    "\n",
    "    var_atr = {}\n",
    "    var_atr['WIDTH'] = col\n",
    "    var_atr['LENGTH'] = row\n",
    "    var_atr['FILE_TYPE'] = 'mask'\n",
    "    var_atr['DATA_TYPE'] = np.float32\n",
    "    writefile.write(variability_scores, out_file = mask_var_score_file , metadata=var_atr)  # writing variability score file\n",
    "else:\n",
    "    variability_scores = readfile.read(mask_var_score_file)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1,2,figsize=(10, 10))\n",
    "# im1 = ax[0].imshow(np.mean(variability_indices, axis=2), cmap='viridis')\n",
    "# fig.colorbar(im1, ax=ax[0], shrink=0.2)\n",
    "# ax[0].set_title('Average temporal variability index map')\n",
    "# im2 = ax[1].imshow(variability_scores, cmap='viridis')\n",
    "# fig.colorbar(im2, ax=ax[1], shrink=0.2)\n",
    "# ax[1].set_title('Temporal variability score map')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "im = ax.imshow(variability_scores, cmap='viridis', interpolation='none')\n",
    "fig.colorbar(im, ax=ax, shrink=0.2)\n",
    "ax.set_title('Temporal variability score map')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thr_var_score = 0.7      # variability score threshold\n",
    "mask_score_flag = True  # variability score mask\n",
    "\n",
    "mask_var_score = variability_scores < thr_var_score     # selecting pixels with small temporal variability score\n",
    "\n",
    "if mask_flag:\n",
    "    for ii in range(n_dates):\n",
    "        insar_displacement[ii,~mask] = np.nan  # applying mask\n",
    "\n",
    "if mask_score_flag:\n",
    "    for ii in range(n_dates):\n",
    "        insar_displacement[ii,~mask_var_score] = np.nan  # applying mask of temporal variability score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if variability masking worked\n",
    "random_inds = np.random.choice(n_dates, num_plot, replace=False)\n",
    "random_inds = sorted(random_inds)\n",
    "\n",
    "fig, ax = plt.subplots(num_plot//3, 3,figsize=(10,10))\n",
    "fig.subplots_adjust(hspace = .001, wspace=.001)\n",
    "\n",
    "axs = ax.ravel()\n",
    "\n",
    "for i in range(num_plot):\n",
    "    axs[i].imshow(insar_displacement[random_inds[i],:,:], cmap='jet',vmin=-50,vmax=50, interpolation='none')     # unit: mm\n",
    "    axs[i].axis('off')\n",
    "    axs[i].set_title(ifgs_date[random_inds[i],0].strftime('%Y%m%d')+'-'+ifgs_date[random_inds[i],1].strftime('%Y%m%d'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminary summary: we have load all data we need for processing:\n",
    "- `atr`: metadata, including incident angle, longitude and latitude step width, etc;\n",
    "- `insar_displacement`: LOS measurement from InSAR;\n",
    "- `insar_coherence`: coherence value of the interferograms:\n",
    "- `ifgs_date`: list of date pairs of two SAR images that form a interferogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transient_gnss_los'></a>\n",
    "# Make GNSS LOS Measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transient_co_gnss'></a>\n",
    "## Find Collocated GNSS Stations\n",
    "The project will have access to L2 position data for continuous GNSS stations in third-party networks such NSF’s Plate Boundary Observatory, the HVO network for Hawaii, GEONET-Japan, and GEONET-New Zealand, located in target regions for NISAR solid earth calval. Station data will be post-processed by one or more analysis centers, will be freely available, and will have latencies of several days to weeks, as is the case with positions currently produced by the NSF’s GAGE Facility and separately by the University of Nevada Reno. Networks will contain one or more areas of high-density station coverage (2~20 km nominal station spacing over 100 x 100 km or more) to support validation of L2 NISAR requirements at a wide range of length scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get space and time range for searching GNSS station:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set AOI\n",
    "analysis_region = sites[site]['analysis_region'].split('\"')[1].split()\n",
    "analysis_region = [float(i) for i in analysis_region]\n",
    "(S,N,W,E) = analysis_region\n",
    "(OG_S,OG_N,OG_W,OG_E) = analysis_region\n",
    "SNWE = [S,N,W,E]\n",
    "OG_SNWE = [S,N,W,E]\n",
    "S_yval, W_xval = ut.coordinate(atr).geo2radar(S, W)[:2]\n",
    "N_yval, E_xval = ut.coordinate(atr).geo2radar(N, E)[:2]\n",
    "# pre-query: convert UTM to lat/lon for query\n",
    "#!#if 'UTM_ZONE' in atr.keys():\n",
    "#!#    S, W = ut0.utm2latlon(atr, SNWE[2], SNWE[0])\n",
    "#!#    N, E = ut0.utm2latlon(atr, SNWE[3], SNWE[1])\n",
    "#!#    SNWE = (S, N, W, E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_gnss = ifgs_date[0,0]\n",
    "end_date_gnss = ifgs_date[-1,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search for collocated GNSS stations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(mintpy_dir)\n",
    "site_names, site_lats_wgs84, site_lons_wgs84 = gnss.search_gnss(SNWE=(S,N,W,E),\n",
    "                                                    start_date=start_date_gnss.strftime('%Y%m%d'),\n",
    "                                                    end_date=end_date_gnss.strftime('%Y%m%d'),\n",
    "                                                    source=gnss_source)\n",
    "\n",
    "# post-query: convert lat/lon to UTM for plotting\n",
    "if 'UTM_ZONE' in atr.keys():\n",
    "    site_lats, site_lons = ut0.latlon2utm(atr, site_lats_wgs84, site_lons_wgs84)\n",
    "else:\n",
    "    site_lats = site_lats_wgs84\n",
    "    site_lons = site_lons_wgs84\n",
    "os.chdir(work_dir)\n",
    "site_names = [str(stn) for stn in site_names]\n",
    "print(\"Initial list of {} stations used in analysis:\".format(len(site_names)))\n",
    "print(site_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transient_gnss_los2'></a>\n",
    "## Make GNSS LOS Measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, the 3-D GNSS observations are projected into LOS direction. The InSAR observations are averaged 3 by 3 near the station positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** the number of pixels used in calculating the averaged phase values at the GPS location depends on the resolution of input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get daily position solutions for GNSS stations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#os.chdir(mint_dir)\n",
    "displacement = {}\n",
    "gnss_time_series = {}\n",
    "gnss_time_series_std = {}\n",
    "bad_stn = {}  #stations to toss\n",
    "pixel_radius = 1   #number of InSAR pixels to average for comparison with GNSS\n",
    "\n",
    "for counter,stn in enumerate(site_names):\n",
    "    gps_obj = GNSS(site = stn,\n",
    "                   data_dir = os.path.join(mintpy_dir,f'GNSS-{gnss_source}'))\n",
    "    gps_obj.open()\n",
    "        \n",
    "    # count number of dates in time range\n",
    "    gps_obj.read_displacement()\n",
    "    dates = gps_obj.dates\n",
    "    for i in range(insar_displacement.shape[0]):\n",
    "        start_date = ifgs_date[i,0]\n",
    "        end_date = ifgs_date[i,-1]\n",
    "        \n",
    "        range_days = (end_date - start_date).days\n",
    "        gnss_count = np.histogram(dates, bins=[start_date,end_date])\n",
    "        gnss_count = int(gnss_count[0])\n",
    "        #print(gnss_count)\n",
    "\n",
    "        # select GNSS stations based on data completeness, here we hope to select stations with data frequency of 1 day and no interruption\n",
    "        if range_days == gnss_count-1:\n",
    "        #if start_date in dates and end_date in dates:\n",
    "            _, disp_gnss_time_series, disp_gnss_time_series_std, site_latlon = gps_obj.get_los_displacement(\n",
    "                str(geom_file),\n",
    "                start_date=start_date.strftime('%Y%m%d'),\n",
    "                end_date=end_date.strftime('%Y%m%d'))[:4]\n",
    "            # post-query: convert lat/lon to UTM\n",
    "            stn_lat = site_latlon[0]\n",
    "            stn_lon = site_latlon[1]\n",
    "            y_value, x_value = ut.coordinate(atr).geo2radar(stn_lat, stn_lon)[:2]\n",
    "            if 'UTM_ZONE' in atr.keys():\n",
    "                stn_lat = float(site_lats_wgs84[counter])\n",
    "                stn_lon = float(site_lons_wgs84[counter])\n",
    "                site_latlon = [stn_lat, stn_lon]\n",
    "\n",
    "            #displacement from insar observation in the gnss station, averaged\n",
    "            #Caution: If you expand the radius parameter farther than the bounding grid it will break. \n",
    "            disp_insar = insar_displacement[i,\n",
    "                                            y_value-pixel_radius:y_value+pixel_radius, \n",
    "                                            x_value-pixel_radius:x_value+pixel_radius]\n",
    "            if np.isfinite(disp_insar).sum() == 0:\n",
    "                break\n",
    "            disp_insar = np.nanmean(disp_insar)\n",
    "\n",
    "            disp_gnss_time_series = disp_gnss_time_series*1000 # convert unit from meter to mm\n",
    "            disp_gnss_time_series_std = disp_gnss_time_series_std*1000\n",
    "            gnss_time_series[(i,stn)] = disp_gnss_time_series\n",
    "            gnss_time_series_std[(i,stn)] = disp_gnss_time_series_std\n",
    "            displacement[(i,stn)] = list(site_latlon)\n",
    "            disp_gnss = disp_gnss_time_series[-1] - disp_gnss_time_series[0]\n",
    "\n",
    "            displacement[(i,stn)].append(disp_gnss)\n",
    "            displacement[(i,stn)].append(disp_insar)\n",
    "        else:\n",
    "            try:\n",
    "                bad_stn[i].append(stn)\n",
    "            except:\n",
    "                bad_stn[i] = [stn]\n",
    "#os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do some data structure transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gnss_time_series = dict(sorted(gnss_time_series.items()))\n",
    "gnss_time_series_std = dict(sorted(gnss_time_series_std.items()))\n",
    "displacement = dict(sorted(displacement.items()))\n",
    "bad_stn = dict(sorted(bad_stn.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In cases cumulative time-series was used, the interval of cumulative time-series is in increments of 6,12,18,\n",
    "# Such that gnss_time_series has differents length for each pairs, and cannot be converted to pandas table.\n",
    "# Capture such cases.\n",
    "try:\n",
    "    gnss_time_series = pd.DataFrame.from_dict(gnss_time_series)\n",
    "    gnss_time_series_std = pd.DataFrame.from_dict(gnss_time_series_std)\n",
    "except ValueError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacement = pd.DataFrame.from_dict(displacement,orient='index',\n",
    "                                      columns=['lat','lon','gnss_disp','insar_disp'])\n",
    "displacement.index = pd.MultiIndex.from_tuples(displacement.index,names=['ifg index','station'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are less than 3 GNSS stations, don't conduct comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "drop_index = []\n",
    "for i in displacement.index.get_level_values(0).unique():\n",
    "    if len(displacement.loc[i]) < 3:\n",
    "        drop_index.append(i)\n",
    "displacement=displacement.drop(drop_index)\n",
    "# ifgs_date after drop for approach 1\n",
    "ifgs_date_ap1=np.delete(ifgs_date,drop_index,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All data needed for approach 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** \n",
    "- A more general criterion is needed for GNSS station selection. Here the stations with uninterrupted data are selected while, in Secular Requirement Validation, stations are selected by data completeness and standard variation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transient_gnss_insar'></a>\n",
    "## Make GNSS and InSAR Relative Displacements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we randomly select one reference site and make both the GNSS and InSAR measurements relative to that reference to remove a constant offset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference GNSS stations to GNSS reference site\n",
    "for i in displacement.index.get_level_values(0).unique():\n",
    "    gps_ref_site_name = random.choice(displacement.loc[i].index.unique())\n",
    "    displacement.loc[i,'gnss_disp'] = displacement.loc[i,'gnss_disp'].values - displacement.loc[(i,gps_ref_site_name),'gnss_disp']\n",
    "    displacement.loc[i,'insar_disp'] = displacement.loc[i,'insar_disp'].values - displacement.loc[(i,gps_ref_site_name),'insar_disp']\n",
    "    # post-query: convert lat/lon to UTM\n",
    "    ref_x_value = displacement.loc[(i,gps_ref_site_name),'lon']\n",
    "    ref_y_value = displacement.loc[(i,gps_ref_site_name),'lat']\n",
    "    ref_y_value, ref_x_value = ut.coordinate(atr).geo2radar(ref_y_value, ref_x_value)[:2]\n",
    "\n",
    "    ref_disp_insar = insar_displacement[i,\n",
    "                                        ref_y_value-pixel_radius:ref_y_value+1+pixel_radius, \n",
    "                                        ref_x_value-pixel_radius:ref_x_value+1+pixel_radius]\n",
    "    ref_disp_insar = np.nanmean(ref_disp_insar)\n",
    "    insar_displacement[i] -= ref_disp_insar\n",
    "    test_disp_insar = np.nanmean(insar_displacement[i,\n",
    "                                        ref_y_value-pixel_radius:ref_y_value+1+pixel_radius, \n",
    "                                        ref_x_value-pixel_radius:ref_x_value+1+pixel_radius])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot GNSS stations on InSAR displacement fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cmap = copy.copy(plt.get_cmap('RdBu'))\n",
    "#cmap.set_bad(color='black')\n",
    "vmin, vmax = np.nanmin(insar_displacement), np.nanmax(insar_displacement)\n",
    "for i in displacement.index.get_level_values(0).unique():\n",
    "    fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "    ifg_iter = insar_displacement[i,\n",
    "                            N_yval:S_yval, \n",
    "                            W_xval:E_xval]\n",
    "    img1 = ax.imshow(ifg_iter, cmap=cmap,vmin=vmin,vmax=vmax, interpolation='nearest', extent=(W, E, S, N))\n",
    "    ax.set_title(ifgs_date[i,0].strftime('%Y%m%d')+'-'+ifgs_date[i,1].strftime('%Y%m%d'))\n",
    "    cbar1 = fig.colorbar(img1, ax=ax)\n",
    "    cbar1.set_label('LOS displacement [mm]')\n",
    "\n",
    "    for stn in displacement.loc[i].index:\n",
    "        lon,lat = displacement.loc[(i,stn),'lon'],displacement.loc[(i,stn),'lat']\n",
    "\n",
    "        color = cmap((displacement.loc[(i,stn),'gnss_disp']-vmin)/(vmax-vmin))\n",
    "        ax.scatter(lon,lat,s=8**2,color=color,edgecolors='k')\n",
    "        ax.annotate(stn,(lon,lat),color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='transient_validation1'></a>\n",
    "# NISAR Validation Approach 1: GNSS-InSAR Direct Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transient_pair1'></a>\n",
    "## Pair up GNSS stations and make measurement residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first pair up all GNSS stations and compare the relative measurement from both GNSS and INSAR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insar_disp = {}\n",
    "gnss_disp = {}\n",
    "ddiff_dist = {}\n",
    "ddiff_disp = {}\n",
    "abs_ddiff_disp = {}\n",
    "for i in displacement.index.get_level_values(0).unique():\n",
    "    displacement_i = displacement.loc[i]\n",
    "    insar_disp_i = []\n",
    "    gnss_disp_i = []\n",
    "    ddiff_dist_i = []\n",
    "    ddiff_disp_i = []\n",
    "\n",
    "    for sta1 in displacement_i.index:\n",
    "        for sta2 in displacement_i.index:\n",
    "            if sta2 == sta1:\n",
    "                break\n",
    "            insar_disp_i.append(displacement_i.loc[sta1,'insar_disp']-displacement_i.loc[sta2,'insar_disp'])\n",
    "            gnss_disp_i.append(displacement_i.loc[sta1,'gnss_disp']-displacement_i.loc[sta2,'gnss_disp'])\n",
    "            ddiff_disp_i.append(gnss_disp_i[-1]-insar_disp_i[-1])\n",
    "            g = pyproj.Geod(ellps=\"WGS84\")\n",
    "            _,_,distance = g.inv(displacement_i.loc[sta1,'lon'],displacement_i.loc[sta1,'lat'],\n",
    "                                 displacement_i.loc[sta2,'lon'],displacement_i.loc[sta2,'lat'])\n",
    "            distance = distance/1000 # convert unit from m to km\n",
    "            ddiff_dist_i.append(distance)\n",
    "    insar_disp[i]=np.array(insar_disp_i)\n",
    "    gnss_disp[i]=np.array(gnss_disp_i)\n",
    "    ddiff_dist[i]=np.array(ddiff_dist_i)\n",
    "    ddiff_disp[i]=np.array(ddiff_disp_i)\n",
    "    abs_ddiff_disp[i]=abs(np.array(ddiff_disp_i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot to compare displacement from GNSS and InSAR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in displacement.index.get_level_values(0).unique():\n",
    "    plt.figure(figsize=(11,7))\n",
    "    disp_range = (min([*insar_disp[i],*gnss_disp[i]]),max([*insar_disp[i],*gnss_disp[i]]))\n",
    "    plt.hist(insar_disp[i],bins=100,range=disp_range,color = \"green\",label='D_InSAR')\n",
    "    plt.hist(gnss_disp[i],bins=100,range=disp_range,color=\"orange\",label='D_GNSS', alpha=0.5)\n",
    "    plt.legend(loc='upper right')\n",
    "    date_range = f'{ifgs_date[i,0].strftime('%Y%m%d')}-{ifgs_date[i,1].strftime('%Y%m%d')}'\n",
    "    plt.title(f\"Displacements \\n Date range {date_range} \\n Number of station pairs used: {len(insar_disp[i])}\")\n",
    "    plt.xlabel('LOS Displacement (mm)')\n",
    "    plt.ylabel('Number of Station Pairs')\n",
    "    plt.savefig(os.path.join(str(mintpy_dir),f'InSARvsGNSS_hist_{date_range}.jpg'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Displacement Residuals Distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in displacement.index.get_level_values(0).unique():\n",
    "    plt.figure(figsize=(11,7))\n",
    "    plt.hist(ddiff_disp[i],bins = 100, color=\"darkblue\",linewidth=1,label='D_gnss - D_InSAR')\n",
    "    plt.legend(loc='upper right')\n",
    "    date_range = f'{ifgs_date[i,0].strftime('%Y%m%d')}-{ifgs_date[i,1].strftime('%Y%m%d')}'\n",
    "    plt.title(f\"Residuals \\n Date range {date_range} \\n Number of stations pairs used: {len(ddiff_disp[i])}\")\n",
    "    plt.xlabel('Displacement Residual (mm)')\n",
    "    plt.ylabel('N Stations')\n",
    "    plt.savefig(os.path.join(str(mintpy_dir),f'InsarvsGNSS_hist_residuals_{date_range}.jpg'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Absolute Displacement Residuals As a Function of Distance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in displacement.index.get_level_values(0).unique():\n",
    "    dist_th = np.linspace(min(ddiff_dist[i]),max(ddiff_dist[i]),100)\n",
    "    acpt_error = 3*(1+np.sqrt(dist_th))\n",
    "    plt.figure(figsize=(11,7))\n",
    "    plt.scatter(ddiff_dist[i],abs_ddiff_disp[i],s=1)\n",
    "    plt.plot(dist_th, acpt_error, 'r')\n",
    "    date_range = f'{ifgs_date[i,0].strftime('%Y%m%d')}-{ifgs_date[i,1].strftime('%Y%m%d')}'\n",
    "    plt.xlabel(\"Distance (km)\")\n",
    "    plt.ylabel(\"Amplitude of Displacement Residuals (mm)\")\n",
    "    plt.title(f\"Residuals \\n Date range {date_range} \\n Number of stations pairs used: {len(ddiff_dist[i])}\")\n",
    "    plt.legend([\"Mission Reqiurement\",\"Measuement\"])\n",
    "    #plt.xlim(0,5)\n",
    "    #plt.savefig(os.path.join(str(mintpy_dir),f'absolute_disp_residuals_{date_range}.jpg'))\n",
    "    #plt.close()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddiff_dist_ap1 = list(ddiff_dist.values())\n",
    "abs_ddiff_disp_ap1 = list(abs_ddiff_disp.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have got all needed data for approach 1:\n",
    "- `ddiff_dist_ap1`: distance of GNSS pairs,\n",
    "- `abs_ddiff_disp_ap1`: absolute value of measurement redisuals,\n",
    "- `ifgs_date_ap1`: list of date pairs of two SAR images that form a interferogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transient_bin1'></a>\n",
    "## Validate the requirement based on binned measurement residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ifgs = len(ddiff_dist_ap1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bin all measurement residuals to check if they pass the requirements or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 10\n",
    "bins = np.linspace(0.1,50.0,num=n_bins+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_all = np.empty([n_ifgs,n_bins+1],dtype=int) # number of points for each ifgs and bins\n",
    "n_pass = np.empty([n_ifgs,n_bins+1],dtype=int) # number of points pass\n",
    "for i in range(n_ifgs):\n",
    "    inds = np.digitize(ddiff_dist_ap1[i],bins)\n",
    "    for j in range(1,n_bins+1):\n",
    "        rqmt = 3*(1+np.sqrt(ddiff_dist_ap1[i][inds==j]))# mission requirement for i-th ifgs and j-th bins\n",
    "        rem = abs_ddiff_disp_ap1[i][inds==j] # relative measurement\n",
    "        assert len(rqmt) == len(rem)\n",
    "        n_all[i,j-1] = len(rem)\n",
    "        n_pass[i,j-1] = np.count_nonzero(rem<rqmt)\n",
    "    n_all[i,-1] = np.sum(n_all[i,0:-2])\n",
    "    n_pass[i,-1] = np.sum(n_pass[i,0:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = n_pass/n_all\n",
    "thresthod = 0.683 \n",
    "#The assumed nature of Gaussian distribution gives a probability of 0.683 of being within one standard deviation.\n",
    "success_or_fail = ratio>thresthod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transient_result1'></a>\n",
    "## Result visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the result to pandas DataFrame for better visulization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_str(x:bool):\n",
    "    if x==True:\n",
    "        return 'true '\n",
    "    elif x==False:\n",
    "        return 'false '\n",
    "\n",
    "success_or_fail_str = [list(map(to_str, x)) for x in success_or_fail]\n",
    "\n",
    "columns = []\n",
    "for i in range(n_bins):\n",
    "    columns.append(f'{bins[i]:.2f}-{bins[i+1]:.2f}')\n",
    "columns.append('total')\n",
    "\n",
    "index = []\n",
    "for i in range(len(ifgs_date_ap1)):\n",
    "    index.append(ifgs_date_ap1[i,0].strftime('%Y%m%d')+'-'+ifgs_date_ap1[i,1].strftime('%Y%m%d'))\n",
    "\n",
    "n_all_pd = pd.DataFrame(n_all,columns=columns,index=index)\n",
    "n_pass_pd = pd.DataFrame(n_pass,columns=columns,index=index)\n",
    "ratio_pd = pd.DataFrame(ratio,columns=columns,index=index)\n",
    "success_or_fail_pd = pd.DataFrame(success_or_fail_str,columns=columns,index=index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of data points in each bin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_all_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of data points that below the curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_pass_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percentage of pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s = ratio_pd.style\n",
    "s.set_table_styles([  # create internal CSS classes\n",
    "    {'selector': '.true', 'props': 'background-color: #e6ffe6;'},\n",
    "    {'selector': '.false', 'props': 'background-color: #ffe6e6;'},\n",
    "], overwrite=False)\n",
    "s.set_td_classes(success_or_fail_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transient_conclusion1'></a>\n",
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = np.count_nonzero(ratio_pd['total']>thresthod)/n_ifgs\n",
    "print(f\"Percentage of interferograms passes the requirement: {percentage}\")\n",
    "if percentage >= 0.70:\n",
    "    print('The interferogram stack passes the requirement.')\n",
    "else:\n",
    "    print('The interferogram stack fails the requirement.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Approach 1 final result for CentralValleyA144: around 79% of interferograms passes the requirement.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transient_validation2'></a>\n",
    "# NISAR Validation Approach 2: Noise Level Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this validation (Approach #2), we evaluate the estimated secular deformation rate (Requirements 658) or co-seismic displacement (Requirement 660) from time series processing or the individual unwrapped interferogram (Requirement 663) over selected cal/val areas with negligible deformation. Any estimated deformation should thus be treated as noise and our goal is to evaluate the significance of this noise. In general, noise in the modeled displacement or the unwrapped interferogram is anisotropic, but here we neglect this anisotropy. Also, we assume the noise is stationary.\n",
    "\n",
    "We first randomly sample measurements and pair up sampled pixel measurements. For each pixel-pair, the difference of their measurement becomes:\n",
    "$$d\\left(r\\right)=|(f\\left(x\\right)-f\\left(x-r\\right))|$$\n",
    "Estimates of $d(r)$ from all pairs are binned according to the distance r. In each bin, $d(r)$ is assumed to be a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Now we simply assume there is no deformation in this study area and time interval. But in fact, it is hard to find a enough large area without any deformation. An more realistic solution is to apply a mask to mask out deformed regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ifgs = insar_displacement.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mask Pixels with Low Coherence (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to loop through to avoid out-of-memory error\n",
    "#for i in range(len(coherenceName)):\n",
    "#    ifgs_coh, _ = readfile.read(ifgs_file,datasetName=coherenceName[i])\n",
    "#    insar_displacement[i][ifgs_coh <0.6] = np.nan\n",
    "#    del ifgs_coh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transient_pair2'></a>\n",
    "## Randomly sample pixels and pair them up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the coordinate for every pixel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 'UTM_ZONE' in atr.keys():\n",
    "    X0,Y0 = load_geo_utm(atr)\n",
    "else:\n",
    "    X0,Y0 = load_geo(atr)\n",
    "X0_2d,Y0_2d = np.meshgrid(X0,Y0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each interferogram, randomly selected pixels need to be paired up. In order to keep measurements independent, different pixel pairs can not share same pixel. This is achieved by pairing up in sequence, i.e., pairing up pixel number 1 and number 2, 3 and 4..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = []; rel_measure = []\n",
    "for i in range(n_ifgs):\n",
    "    dist_i, rel_measure_i = samp_pair(X0_2d,Y0_2d,insar_displacement[i],num_samples=1000000)\n",
    "    dist.append(dist_i)\n",
    "    rel_measure.append(rel_measure_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the statistical property of selected pixel pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(n_ifgs):\n",
    "    fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "    img1 = ax.hist(dist[i], bins=100)\n",
    "    ax.set_title(f\"Histogram of distance \\n Date range {ifgs_date[i,0].strftime('%Y%m%d')}-{ifgs_date[i,1].strftime('%Y%m%d')}\")\n",
    "    ax.set_xlabel(r'Distance ($km$)')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_xlim(0,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(n_ifgs):\n",
    "    fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "    img1 = ax.hist(rel_measure[i], bins=100)\n",
    "    ax.set_title(f\"Histogram of Relative Measurement \\n Date range {ifgs_date[i,0].strftime('%Y%m%d')}-{ifgs_date[i,1].strftime('%Y%m%d')}\")\n",
    "    ax.set_xlabel(r'Relative Measurement ($mm$)')\n",
    "    ax.set_ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dist_th = np.linspace(0,50,100)\n",
    "rqmt = 3*(1+np.sqrt(dist_th))\n",
    "for i in range(n_ifgs):\n",
    "    fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "    ax.plot(dist_th, rqmt, 'r')\n",
    "    ax.scatter(dist[i], rel_measure[i], s=1, alpha=0.25)\n",
    "    ax.set_title(f\"Comparison between Relative Measurement and Requirement Curve \\n Date range {ifgs_date[i,0].strftime('%Y%m%d')}-{ifgs_date[i,1].strftime('%Y%m%d')}\")\n",
    "    ax.set_ylabel(r'Relative Measurement ($mm$)')\n",
    "    ax.set_xlabel('Distance (km)')\n",
    "    ax.set_xlim(0,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have got data used of approach 2:\n",
    "- `dist`: distance of pixel pairs,\n",
    "- `rel_measure`: relative measurement of pixel pairs,\n",
    "- `ifgs_date`: list of date pairs of two SAR images that form a interferogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transient_bin2'></a>\n",
    "## Validate the requirement based on binned measurement residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ifgs = len(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bin all measurement residuals to check if they pass the requirements or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 10\n",
    "bins = np.linspace(0.1,50.0,num=n_bins+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_all = np.empty([n_ifgs,n_bins+1],dtype=int) # number of points for each ifgs and bins\n",
    "n_pass = np.empty([n_ifgs,n_bins+1],dtype=int) # number of points pass\n",
    "#ratio = np.empty([n_ifgs,n_bins+1]) # ratio\n",
    "# the final column is the ratio as a whole\n",
    "for i in range(n_ifgs):\n",
    "    inds = np.digitize(dist[i],bins)\n",
    "    for j in range(1,n_bins+1):\n",
    "        rqmt = 3*(1+np.sqrt(dist[i][inds==j]))# mission requirement for i-th ifgs and j-th bins\n",
    "        rem = rel_measure[i][inds==j] # relative measurement\n",
    "        assert len(rqmt) == len(rem)\n",
    "        n_all[i,j-1] = len(rem)\n",
    "        n_pass[i,j-1] = np.count_nonzero(rem<rqmt)\n",
    "    n_all[i,-1] = np.sum(n_all[i,0:-2])\n",
    "    n_pass[i,-1] = np.sum(n_pass[i,0:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = n_pass/n_all\n",
    "mean_ratio = np.array([np.mean(ratio[:,:-1],axis=1)])\n",
    "ratio = np.hstack((ratio,mean_ratio.T))\n",
    "thresthod = 0.683\n",
    "#The assumed nature of Gaussian distribution gives a probability of 0.683 of being within one standard deviation.\n",
    "success_or_fail = ratio>thresthod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transient_result2'></a>\n",
    "## Result visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the result to pandas DataFrame for better visulization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_str(x:bool):\n",
    "    if x==True:\n",
    "        return 'true '\n",
    "    elif x==False:\n",
    "        return 'false '\n",
    "\n",
    "success_or_fail_str = [list(map(to_str, x)) for x in success_or_fail]\n",
    "\n",
    "columns = []\n",
    "for i in range(n_bins):\n",
    "    columns.append(f'{bins[i]:.2f}-{bins[i+1]:.2f}')\n",
    "columns.append('total')\n",
    "\n",
    "index = []\n",
    "for i in range(len(ifgs_date)):\n",
    "    index.append(ifgs_date[i,0].strftime('%Y%m%d')+'-'+ifgs_date[i,1].strftime('%Y%m%d'))\n",
    "\n",
    "n_all_pd = pd.DataFrame(n_all,columns=columns,index=index)\n",
    "n_pass_pd = pd.DataFrame(n_pass,columns=columns,index=index)\n",
    "ratio_pd = pd.DataFrame(ratio,columns=columns+['mean'],index=index)\n",
    "success_or_fail_pd = pd.DataFrame(success_or_fail_str,columns=columns+['mean'],index=index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of data points in each bin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_all_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of data points that below the curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_pass_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ratio of pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s = ratio_pd.style\n",
    "s.set_table_styles([  # create internal CSS classes\n",
    "    {'selector': '.true', 'props': 'background-color: #e6ffe6;'},\n",
    "    {'selector': '.false', 'props': 'background-color: #ffe6e6;'},\n",
    "], overwrite=False)\n",
    "s.set_td_classes(success_or_fail_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transient_conclusion2'></a>\n",
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared with percentage of total passed pairs, the mean value of percentage of passed pairs in all bin is a better indicator since it gives all bins same weight. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = np.count_nonzero(ratio_pd['mean']>thresthod)/n_ifgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Percentage of interferograms passes the requirement (70%): {percentage}.\")\n",
    "if percentage >= 0.70:\n",
    "    print('The interferogram stack passes the requirement.')\n",
    "else:\n",
    "    print('The interferogram stack fails the requirement.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Approach 2 final result for CentralValleyA144: 100% of interferograms passes the requirement.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transient_appendix'></a>\n",
    "# Appendix: GNSS Position Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GNSS vs InSAR scatter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gps_ref_site_name = sites[site]['gps_ref_site_name']\n",
    "for stn in site_names:\n",
    "    date_list = []\n",
    "    gnss_series = []\n",
    "    insar_series = []\n",
    "    for i in range(n_ifgs):\n",
    "        # catch cases where GPS site has been filtered out\n",
    "        try:\n",
    "            ref_gnss_dat = displacement.loc[(i,gps_ref_site_name),'gnss_disp']\n",
    "            ref_insar_dat = displacement.loc[(i,gps_ref_site_name),'insar_disp']\n",
    "            gnss_series.append(displacement.loc[(i,str(stn)),'gnss_disp'] - ref_gnss_dat)\n",
    "            insar_series.append(displacement.loc[(i,str(stn)),'insar_disp'] - ref_insar_dat)\n",
    "            date_list.append(ifgs_date[i,1].strftime('%Y%m%d'))\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    # only plot for cases where GPS site has not been filtered out\n",
    "    if date_list != [] and gnss_series != [] and insar_series != []:\n",
    "        dt_date_list = [dt.strptime(date, '%Y%m%d').strftime('%Y-%m-%d') for date in date_list]\n",
    "        # plot station TS\n",
    "        fig, ax = plt.subplots(figsize=(15, 5))\n",
    "        ax.set_title(f\"station name: {stn}\")\n",
    "        ax.scatter(dt_date_list, insar_series, c='#ff7f0e', label=\"InSAR\")\n",
    "        ax.scatter(dt_date_list, gnss_series, c='#1f77b4', label=\"GNSS\", alpha=0.7)\n",
    "        ax.set_xlabel('Time')\n",
    "        ax.set_ylabel('Relative position in LOS direction (mm)')\n",
    "        ax.xaxis.set_major_locator(MaxNLocator(nbins=5))\n",
    "        ax.legend()\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
