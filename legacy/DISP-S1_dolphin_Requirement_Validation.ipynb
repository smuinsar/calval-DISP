{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow to Validate NISAR L2 Secular Displacement Requirement\n",
    "\n",
    "**Original code authored by:** David Bekaert, Heresh Fattahi, Eric Fielding, and Zhang Yunjun with \n",
    "Extensive modifications by Adrian Borsa and Amy Whetter and other NISAR team members 2022\n",
    "\n",
    "**Updated for OPERA requirements by Simran Sangha, Marin Govorcin, and Al Handwerger**\n",
    "\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "Both the initial setup (<b>Prep A</b> section) and download of the data (<b>Prep B</b> section) should be run at the start of the notebook. And all subsequent sections NEED to be run in order.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define CalVal Site "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters for papermill\n",
    "\n",
    "### Choose a site from the 'sites' dictionary found 2 cells down\n",
    "## If your des_D087 area is not defined, add a new dictionary entry as appropriate and provide a unique site keyname\n",
    "site = 'des_D115'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# injected parameters\n",
    "\n",
    "### Choose a site from the 'sites' dictionary found 2 cells down\n",
    "## If your study area is not defined, add a new dictionary entry as appropriate and provide a unique site keyname\n",
    "site = 'des_D087'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Manually specify the appropriate dolphin and mintpy output directories, if they exist\n",
    "dolphin_dir = 'dolphin_run'\n",
    "mintpy_dir = 'MintPy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define list of requirements\n",
    "## Static for OPERA Cal/Val requirements, do not touch!\n",
    "\n",
    "findMax = 'false' # set to 'true' if you want to find the maximum threshold, set to 'false' if you want to find the minimum threshold\n",
    "\n",
    "# Define secular requirements\n",
    "secular_gnss_rqmt = 5  # mm/yr for 3 years of data over length scales of 0.1-50 km\n",
    "gnss_dist_rqmt = [0.1, 50.0]  # km\n",
    "secular_insar_rqmt = 5  # mm/yr\n",
    "insar_dist_rqmt = [0.1, 50.0]  # km\n",
    "\n",
    "# Define temporal sampling requirement\n",
    "insar_sampling = 12 # days\n",
    "insar_sampling_percentage = 80 # percentage of acquitions at 12 day sampling (insar_sampling) or better\n",
    "insar_timespan_requirement = 4 # years\n",
    "\n",
    "# Define spatial coherence threshold (necessary to reject poor quality, long temporal baseline pairs)\n",
    "coherenceBased_parm = 'yes'\n",
    "minCoherence_parm = '0.4'\n",
    "\n",
    "# Set mask file\n",
    "maskFile = 'maskSpatialCoh.h5' # maskTempCoh.h5 maskConnComp.h5 waterMask.h5 (maskConnComp.h5 is very conservative)\n",
    "water_mask = 'esa_world_cover_2021'\n",
    "\n",
    "# set DEM file\n",
    "dem_file = 'glo_30'\n",
    "\n",
    "# specify number of InSAR pixels to average for comparison with GNSS\n",
    "pixel_radius = 5\n",
    "\n",
    "# specify GNSS source for validation\n",
    "from mintpy.objects import gnss\n",
    "gnss_source = 'UNR'\n",
    "print(f'Searching for all GNSS stations from source: {gnss_source}')\n",
    "print(f'May use any of the following supported sources: {gnss.GNSS_SOURCES}')\n",
    "GNSS = gnss.get_gnss_class(gnss_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### List of CalVal Sites:\n",
    "'''\n",
    "Set NISAR calval sites:\n",
    "    des_D087  : Hawaii - Descending track 87\n",
    "\n",
    "dolphin & MintPy parameters:\n",
    "    calval_location : name\n",
    "    download_region : download box in S,N,W,E format\n",
    "    analysis_region : analysis box in S,N,W,E format (must be within download_region)\n",
    "    reference_lalo : latitute,longitude in geographic coordinates (default: None)\n",
    "    download_start_date : download start date as YYYMMDD  \n",
    "    download_end_date   : download end date as YYYMMDD\n",
    "    flight_dir: flight direction ('asc' or 'des')\n",
    "    gps_ref_site_name : Name of the GPS site for InSAR re-referencing\n",
    "    water_mask : water mask type to download\n",
    "    dem_file : DEM file type to download\n",
    "'''\n",
    "sites = {\n",
    "    ##########  des_D087 ##############\n",
    "    'des_D087' : {'calval_location' : 'smallbaselineApp',\n",
    "            'download_region' : '\"19.0 20.1962 -155.88 -155.014\"', # download box in S,N,W,E format\n",
    "            'analysis_region' : '\"19.0 20.1962 -155.88 -155.014\"', # analysis box in S,N,W,E format (must be within download_region)\n",
    "            'reference_lalo' : '19.2485991551617 -155.32285148610057',\n",
    "            'download_start_date' : '20220701',\n",
    "            'download_end_date' : '20230701',\n",
    "            'flight_dir' : 'des',\n",
    "            'gps_ref_site_name' : 'KULE', # reference site for this area (-155.564 19.322)\n",
    "            'use_staged_data': True, # option to control the use of pre-staged data; [False/True]\n",
    "            'water_mask': water_mask,\n",
    "            'dem_file': dem_file,\n",
    "            'vmin' : -30,\n",
    "            'vmax' : 30}\n",
    "}\n",
    "secular_available_sites = list(sites.keys())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Table of Contents:\n",
    "<a id='secular_TOC'></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "[**Prep A. Environment Setup**](#secular_prep_a)\n",
    "\n",
    "[**Prep B. Data Staging**](#secular_prep_b)\n",
    "\n",
    "[**1. Generate Interferogram Stack**](#secular_gen_ifg)\n",
    "- [1.1.  Generate interferograms using dolphin](#secular_crop_ifg)\n",
    "\n",
    "[**2. Generation of Time Series from Interferograms**](#secular_gen_ts)\n",
    "- [2.1. Set Up MintPy Configuration file](#secular_setup_config)\n",
    "- [2.2. Load Data into MintPy](#secular_load_data)\n",
    "- [2.3. Generate Quality Control Mask](#secular_generate_mask)\n",
    "\n",
    "[**3. Optional Corrections**](#secular_opt_correction)\n",
    "- [3.1. Topographic Residual Correction ](#secular_topo_corr) \n",
    "\n",
    "[**4. Estimate InSAR and GNSS Velocities**](#secular_decomp_ts)\n",
    "- [4.1. Estimate InSAR LOS Velocities](#secular_insar_vel1)\n",
    "- [4.2. Find Collocated GNSS Stations](#secular_co_gps)  \n",
    "- [4.3. Get GNSS Position Time Series](#secular_gps_ts) \n",
    "- [4.4. Make GNSS LOS Velocities](#secular_gps_los)\n",
    "- [4.5. Re-Reference GNSS and InSAR Velocities](#secular_gps_insar)\n",
    "\n",
    "[**5. NISAR Validation Approach 1: GNSS-InSAR Direct Comparison**](#secular_nisar_validation)\n",
    "- [5.1. Make Velocity Residuals at GNSS Locations](#secular_make_vel)\n",
    "- [5.2. Make Double-differenced Velocity Residuals](#secular_make_velres)\n",
    "- [5.3. Secular Requirement Validation: Method 1](#secular_valid_method1)\n",
    "\n",
    "[**6. NISAR Validation Approach 2: InSAR-only Structure Function**](#secular_nisar_validation2)\n",
    "- [6.1. Read Array and Mask Pixels with no Data](#secular_array_mask)\n",
    "- [6.2. Randomly Sample Pixels and Pair Them Up with Option to Remove Trend](#secular_remove_trend)\n",
    "- [6.3. Amplitude vs. Distance of Relative Measurements (pair differences)](#secular_M2ampvsdist2)\n",
    "- [6.4. Bin Sample Pairs by Distance Bin and Calculate Statistics](#secular_M2RelMeasTable)\n",
    "\n",
    "[**Appendix: Supplementary Comparisons and Plots**](#secular_appendix1)\n",
    "- [A.1. Compare Raw Velocities](#secular_compare_raw)\n",
    "- [A.2. Plot Velocity Residuals](#secular_plot_vel)\n",
    "- [A.3. Plot Double-differenced Residuals](#secular_plot_velres)\n",
    "- [A.4. GPS Position Plot](#secular_appendix_gps)\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_prep_a'></a>\n",
    "## Prep A. Environment Setup\n",
    "Setup your environment for processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Packages\n",
    "import glob\n",
    "import os\n",
    "import subprocess\n",
    "from datetime import datetime as dt\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from mintpy.objects import gnss, timeseries\n",
    "from mintpy.objects.gnss import search_gnss\n",
    "from mintpy.smallbaselineApp import TimeSeriesAnalysis\n",
    "from mintpy.utils import ptime, readfile, utils as ut, utils0 as ut0\n",
    "from mintpy.cli import view, plot_network, reference_point, generate_mask, tropo_pyaps3, diff\n",
    "from scipy import signal\n",
    "\n",
    "from solid_utils.sampling import load_geo, load_geo_utm, samp_pair, profile_samples, haversine_distance\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "#Set Global Plot Parameters\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "################# Set Directories ##########################################\n",
    "print('\\nCurrent directory:',os.getcwd())\n",
    "\n",
    "# set path to aria-tools/MintPy/FRInGE output (if already generated)\n",
    "work_dir = os.getcwd()\n",
    "\n",
    "work_dir = Path(work_dir)\n",
    "\n",
    "print(\"Work directory:\", work_dir)\n",
    "work_dir.mkdir(parents=True, exist_ok=True)\n",
    "if not os.path.exists(dolphin_dir):\n",
    "    dolphin_dir = os.path.join(work_dir, 'dolphin_run')\n",
    "dolphin_dir = os.path.abspath(dolphin_dir)\n",
    "# Change to Workdir\n",
    "os.chdir(work_dir)\n",
    "\n",
    "if not os.path.exists(mintpy_dir):\n",
    "    mintpy_dir = work_dir/'MintPy'\n",
    "    mintpy_dir.mkdir(parents=True, exist_ok=True)\n",
    "mintpy_dir = os.path.abspath(mintpy_dir)\n",
    "print(\"MintPy  dir:\", mintpy_dir)\n",
    "vel_file = os.path.join(mintpy_dir, 'velocity.h5')\n",
    "insar_ts_file = os.path.join(mintpy_dir, 'timeseries.h5')\n",
    "msk_file = os.path.join(mintpy_dir, maskFile)  # maskTempCoh.h5 maskSpatialCoh.h5 maskConnComp.h5 waterMask.h5\n",
    "\n",
    "if site not in secular_available_sites:\n",
    "    msg = '\\nSelected site not available! Please select one of the following sites:: \\n{}'.format(secular_available_sites)\n",
    "    raise Exception(msg)\n",
    "else:\n",
    "    print('\\nSelected site: {}'.format(site))\n",
    "    for key, value in sites[site].items():\n",
    "        print('   '+ key, ' : ', value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_gen_ifg'></a>\n",
    "# 1. Generate Interferogram Stack\n",
    "\n",
    "We leverage the dolphin software, which generates high resolution interferometric products using combined PS/DS processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_crop_ifg'></a>\n",
    "## 1. Generate interferograms using dolphin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop Interferograms to Analysis Region\n",
    "if not sites[site]['use_staged_data']:\n",
    "    ###########################################################################################################\n",
    "    # Run dolphin and mask data with esa world cover water mask:\n",
    "    if (not os.path.exists(dolphin_dir)\n",
    "        and not os.path.exists(vel_file)):\n",
    "        print('Preparing IFGs for MintPY....')\n",
    "        command = (\n",
    "            f\"pst_dolphin_workflow.py \"\n",
    "            f\"-s {sites[site]['download_start_date']} -e {sites[site]['download_end_date']} \"\n",
    "            f\"-ao {sites[site]['analysis_region']} -op {sites[site]['flight_dir']} \"\n",
    "            f\"--water-mask-file {sites[site]['water_mask']} --dem-file {sites[site]['dem_file']} \"\n",
    "            f\"-o {dolphin_dir} \"\n",
    "            f\"--unwmethod phass --threadsperworker 4 --nparalleljobs 2 \"\n",
    "            f\"--nparalleltiles 2 --ntiles 2\"\n",
    "        )\n",
    "\n",
    "        ################################## PREPARE STACK ###################################################\n",
    "        print(command)\n",
    "        result = subprocess.run(command, stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT, text=True, shell=True)\n",
    "    print('Finish preparing IFGs for MintPy!!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_gen_ts'></a>\n",
    "# 2. Generation of Time Series from Interferograms\n",
    "\n",
    "InSAR time series (i.e., the unfiltered displacement of each pixel vs. time) are estimated from a processed InSAR stack from Section 3.1 (either ascending or descending) using a variant of the small baseline subset (SBAS) approach and then parameterized using the approach described in Section 4. This step uses tools available in the MintPy software package (Yunjun et al. (2019)), which provides both SBAS time series and model-based time series parameterization. Recent results on InSAR closure phase and “fading signal” recommend the of use all available interferograms to avoid systematic bias (_Ansari et al._, 2020; _Zheng Y.J. et al._, 2022). As we expect high-quality orbital control for NISAR, we anticipate that the interferogram stack will typically include all nearest-neighbor (i.e., ~12-day pairs) and skip-1 interferograms, which will be the minimum inputs into the SBAS generation step."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='secular_load_data'></a>\n",
    "## 2.1. Load Data into MintPy\n",
    "\n",
    "The output of this step is an \"inputs\" directory in 'calval_directory/calval_location/MintPy/\" containing two HDF5 files:\n",
    "- ifgramStack.h5: This file contains 6 dataset cubes (e.g. unwrapped phase, coherence, connected components etc.) and multiple metadata\n",
    "- geometryGeo.h5: This file contains geometrical datasets (e.g., incidence/azimuth angle, masks, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(vel_file):\n",
    "    command = (\n",
    "        f\"prep_mintpy.py -m {dolphin_dir}/static_CSLCs/ \"\n",
    "        f\"-c \\\"{dolphin_dir}/dolphin_output/stitched_interferograms/*int.cor.zeroed.cor.tif\\\" \"\n",
    "        f\"-u \\\"{dolphin_dir}/dolphin_output/stitched_interferograms/*.unw.zeroed.tif\\\" \"\n",
    "        f\"--geom-dir {dolphin_dir}/dolphin_output/stitched_interferograms/geometry \"\n",
    "        f\"--ref-lalo \\\"{sites[site]['reference_lalo']}\\\" \"\n",
    "        f\"--single-reference -o {mintpy_dir}\"\n",
    "    )\n",
    "    print('command', command)\n",
    "    process = subprocess.run(command, shell=True)\n",
    "\n",
    "### Change to MintPy workdir\n",
    "os.chdir(mintpy_dir)\n",
    "print('Mintpy input files:')\n",
    "[x for x in os.listdir('.') if x.endswith('.h5')]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='secular_generate_mask'></a>\n",
    "## 2.2. Generate Quality Control Mask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mask files can be can be used to mask pixels in the time-series processing. Below we generate a mask file based on the connected components, which is a metric for unwrapping quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(maskFile):\n",
    "    iargs = ['avgSpatialCoh.h5', '-m', minCoherence_parm, '-o', maskFile]\n",
    "    generate_mask.main(iargs)\n",
    "    # view mask\n",
    "    view.main([maskFile, 'mask'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='secular_common_latlon'></a>\n",
    "## 2.3. Reference Interferograms To Common Lat/Lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sites[site]['reference_lalo']:\n",
    "    lat = sites[site]['reference_lalo'].split()[0]\n",
    "    lon = sites[site]['reference_lalo'].split()[1]\n",
    "    iargs = [insar_ts_file, '-l', lat, '-L', lon]\n",
    "    reference_point.main(iargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get date list\n",
    "date_list = timeseries(insar_ts_file).get_date_list()\n",
    "num_date = len(date_list)\n",
    "date0, date1 = date_list[0], date_list[-1]\n",
    "insar_dates = ptime.date_list2vector(date_list)[0]\n",
    "\n",
    "# Check temporal sampling\n",
    "insar_sampling_arr = []\n",
    "for i in range(len(insar_dates)-1):\n",
    "    diff = (insar_dates[i+1] - insar_dates[i]).days\n",
    "    insar_sampling_arr.append(diff)\n",
    "\n",
    "count = 0\n",
    "for i in insar_sampling_arr:\n",
    "    if i <= insar_sampling:\n",
    "        count += 1\n",
    "\n",
    "percentage = (count / len(insar_sampling_arr)) * 100\n",
    "timespan_of_insar=(insar_dates[len(insar_dates)-1]-insar_dates[0]).days /365.25\n",
    "\n",
    "# Overall pass/fail criterion\n",
    "if percentage >= insar_sampling_percentage:\n",
    "    print(f'This velocity dataset ({percentage}%) passes the temporal sampling requirement ({insar_sampling_percentage}%)')\n",
    "else:\n",
    "    print(f'This velocity dataset ({percentage}%) does NOT pass the temporal sampling requirement ({insar_sampling_percentage}%)')\n",
    "\n",
    "if timespan_of_insar >= insar_timespan_requirement:\n",
    "    print(f'This velocity dataset ({timespan_of_insar} years) passes the timespan requirement ({insar_timespan_requirement} years)')\n",
    "else:\n",
    "    print(f'This velocity dataset ({timespan_of_insar} years) does NOT pass the timespan requirement ({insar_timespan_requirement } years)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='secular_opt_correction'></a>\n",
    "# 3. Optional Corrections"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='secular_tropo_corr'></a>\n",
    "## 3.1. Tropospheric Delay Correction\n",
    "\n",
    "Optional atmospheric correction utilizes the PyAPS (Jolivet et al., 2011, Jolivet and Agram, 2012) module within GIAnT (or eventually a merged replacement for GIAnT and MintPy). PyAPS is well documented, maintained and can be freely downloaded. PyAPS is included in GIAnT distribution). PyAPS currently includes support for ECMWF’s ERA-Interim, NOAA’s NARR and NASA’s MERRA weather models. A final selection of atmospheric models to be used for operational NISAR processing will be done during Phase C.\n",
    "\n",
    "[T]ropospheric delay maps are produced from atmospheric data provided by Global Atmospheric Models. This method aims to correct differential atmospheric delay correlated with the topography in interferometric phase measurements. Global Atmospheric Models (hereafter GAMs)... provide estimates of the air temperature, the atmospheric pressure and the humidity as a function of elevation on a coarse resolution latitude/longitude grid. In PyAPS, we use this 3D distribution of atmospheric variables to determine the atmospheric phase delay on each pixel of each interferogram.\n",
    "\n",
    "The absolute atmospheric delay is computed at each SAR acquisition date. For a pixel a_i at an elevation z at acquisition date i, the four surrounding grid points are selected and the delays for their respective elevations are computed. The resulting delay at the pixel a_i is then the bilinear interpolation between the delays at the four grid points. Finally, we combine the absolute delay maps of the InSAR partner images to produce the differential delay maps used to correct the interferograms.\n",
    "\n",
    "[MintPy provides functionality for this correction.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "do_tropo_correction = False\n",
    "########################################################################\n",
    "'''\n",
    "REFERENCE : https://github.com/insarlab/pyaps#2-account-setup-for-era5\n",
    "Read Section 2 for ERA5 [link above] to create an account on the CDS website.\n",
    "'''\n",
    "\n",
    "if do_tropo_correction:\n",
    "    #!#if not sites[site]['use_staged_data'] and not os.path.exists(Path.home()/'.cdsapirc'):\n",
    "        #!#print('NEEDED to download ERA5, link: https://cds.climate.copernicus.eu/user/register')\n",
    "        #UID = input('Please type your CDS_UID:')\n",
    "        #CDS_API = input('Please type your CDS_API:')\n",
    "        \n",
    "        #!#cds_tmp = '''url: https://cds.climate.copernicus.eu/api/v2\n",
    "        #!#key: {UID}:{CDS_API}'''.format(UID=UID, CDS_API=CDS_API)\n",
    "        #!#os.system('echo \"{cds_tmp}\" > ~/.cdsapirc; chmod 600 ~/.cdsapirc'.format(cds_tmp = str(cds_tmp)))\n",
    "    \n",
    "    # get ERA5 delays\n",
    "    iargs = [insar_ts_file, '-g', 'geometryGeo.h5', '-m ERA5']\n",
    "    tropo_pyaps3.main(iargs)\n",
    "    \n",
    "    view.main(['ERA5.h5'])\n",
    "\n",
    "    # apply diff to TS\n",
    "    iargs = [insar_ts_file, 'ERA5.h5', '-o', 'timeseries_ERA5.h5']\n",
    "    diff.main(iargs)\n",
    "    insar_ts_file = 'timeseries_ERA5.h5'\n",
    "\n",
    "else:\n",
    "    insar_ts_file = 'timeseries.h5'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_decomp_ts'></a>\n",
    "# 4. Estimate InSAR and GNSS Velocities\n",
    "The approach that will be used for the generation of NISAR L3 products for Requirements 660 and 663 allows for an explicit inclusion of key basis functions (e.g., Heaviside functions, secular rate, etc.) in the InSAR inversion. Modifications to this algorithm may be identified and implemented in response to NISAR Phase C activities. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_insar_vel1'></a>\n",
    "## 4.1. Estimate InSAR LOS Velocities\n",
    "\n",
    "Given a time series of InSAR LOS displacements, the observations for a given pixel, $U(t)$, can be parameterized as:\n",
    "\n",
    "$$U(t) = a \\;+\\; vt \\;+\\; c_1 cos (\\omega_1t - \\phi_{1,}) \\;+\\; c_2 cos (\\omega_2t - \\phi_2) \\;+\\; \\sum_{j=1}^{N_{eq}} \\left( h_j+f_j F_j (t-t_j) \\right)H(t - t_j) \\;+\\; \\frac{B_\\perp (t)}{R sin \\theta}\\delta z \\;+\\; residual$$ \n",
    "\n",
    "which includes a constant offset $(a)$, velocity $(v)$, and amplitudes $(c_j)$ and phases $(\\phi_j)$ of annual $(\\omega_1)$ and semiannual $(\\omega_2)$ sinusoidal terms.  Where needed we can include additional complexity, such as coseismic and postseismic processes parameterized by Heaviside (step) functions $H$ and postseismic functions $F$ (the latter typically exponential and/or logarithmic).   $B_\\perp(t)$, $R$, $\\theta$, and $\\delta z$ are, respectively, the perpendicular component of the interferometric baseline relative to the first date, slant range distance, incidence angle and topography error correction for the given pixel. \n",
    "\n",
    "Thus, given either an ensemble of interferograms or the output of SBAS (displacement vs. time), we can write the LSQ problem as \n",
    "\n",
    "$$ \\textbf{G}\\textbf{m} = \\textbf{d}$$\n",
    "\n",
    "where $\\textbf{G}$ is the design matrix (constructed out of the different functional terms in Equation 2 evaluated either at the SAR image dates for SBAS output, or between the dates spanned by each pair for interferograms), $\\textbf{m}$ is the vector of model parameters (the coefficients in Equation 2) and $\\textbf{d}$ is the vector of observations.  For GPS time series, $\\textbf{G}, \\textbf{d}, \\textbf{m}$ are constructed using values evaluated at single epochs corresponding to the GPS solution times, as for SBAS InSAR input. \n",
    "\n",
    "With this formulation, we can obtain InSAR velocity estimates and their formal uncertainties (including in areas where the expected answer is zero). \n",
    "\n",
    "The default InSAR velocity fit in MintPy is to estimate a mean linear velocity $(v)$ in in the equation, which we do below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load velocity file\n",
    "insar_velocities,_ = readfile.read(vel_file, datasetName = 'velocity')  # read velocity file\n",
    "insar_velocities = insar_velocities * 1000.  # convert velocities from m to mm\n",
    "\n",
    "# set masked pixels to NaN\n",
    "msk,_ = readfile.read(msk_file)\n",
    "insar_velocities[msk == 0] = np.nan\n",
    "insar_velocities[insar_velocities == 0] = np.nan"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the mean linear velocity fit. The MintPy `view` module automatically reads the temporal coherence mask `maskTempCoh.h5` and applies that to mask out pixels with unreliable velocities (white)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scp_args = 'velocity.h5 velocity -v -20 20 --colormap RdBu_r --figtitle LOS_Velocity --unit mm/yr -m ' + msk_file\n",
    "view.main(scp_args.split())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Note :</b> \n",
    "Negative values indicates that target is moving away from the radar (i.e., Subsidence in case of vertical deformation).\n",
    "Positive values indicates that target is moving towards the radar (i.e., uplift in case of vertical deformation). \n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_co_gps'></a>\n",
    "## 4.2. Find Collocated GNSS Stations\n",
    "\n",
    "The project will have access to L2 position data for continuous GNSS stations in third-party networks such NSF’s Plate Boundary Observatory, the HVO network for Hawaii, GEONET-Japan, and GEONET-New Zealand, located in target regions for NISAR solid earth calval. Station data will be post-processed by one or more analysis centers, will be freely available, and will have latencies of several days to weeks, as is the case with positions currently produced by the NSF’s GAGE Facility and separately by the University of Nevada Reno. Networks will contain one or more areas of high-density station coverage (2~20 km nominal station spacing over 100 x 100 km or more) to support validation of L2 NISAR requirements at a wide range of length scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get analysis metadata from InSAR velocity file\n",
    "insar_metadata = readfile.read_attribute(vel_file)\n",
    "lat_step = float(insar_metadata['Y_STEP'])\n",
    "lon_step = float(insar_metadata['X_STEP'])\n",
    "(S,N,W,E) = ut.four_corners(insar_metadata)\n",
    "(OG_S,OG_N,OG_W,OG_E) = ut.four_corners(insar_metadata)\n",
    "SNWE = [S,N,W,E]\n",
    "OG_SNWE = [S,N,W,E]\n",
    "# pre-query: convert UTM to lat/lon for query\n",
    "if 'UTM_ZONE' in insar_metadata.keys():\n",
    "    S, W = ut0.utm2latlon(insar_metadata, SNWE[2], SNWE[0])\n",
    "    N, E = ut0.utm2latlon(insar_metadata, SNWE[3], SNWE[1])\n",
    "    SNWE = (S, N, W, E)\n",
    "\n",
    "start_date = insar_metadata.get('START_DATE', None)\n",
    "end_date = insar_metadata.get('END_DATE', None)\n",
    "start_date_gnss = dt.strptime(start_date, \"%Y%m%d\")\n",
    "end_date_gnss = dt.strptime(end_date, \"%Y%m%d\")\n",
    "\n",
    "geom_file = os.path.join(mintpy_dir, 'geometryGeo.h5')\n",
    "inc_angle = readfile.read(geom_file, datasetName='incidenceAngle')[0]\n",
    "inc_angle = np.nanmean(inc_angle)\n",
    "az_angle = readfile.read(geom_file, datasetName='azimuthAngle')[0]\n",
    "az_angle = np.nanmean(az_angle)\n",
    "\n",
    "#Set GNSS Parameters\n",
    "gps_completeness_threshold = 0.9    #0.9  #percent of data timespan with valid GNSS epochs\n",
    "gps_residual_stdev_threshold = 10.  #0.03  #0.03  #max threshold standard deviation of residuals to linear GNSS fit\n",
    "\n",
    "# search for collocated GNSS stations\n",
    "site_names, site_lats_wgs84, site_lons_wgs84 = gnss.search_gnss(SNWE=(S,N,W,E),\n",
    "                                                                start_date=start_date,\n",
    "                                                                end_date=end_date,\n",
    "                                                                source=gnss_source)\n",
    "# post-query: convert lat/lon to UTM for plotting\n",
    "if 'UTM_ZONE' in insar_metadata.keys():\n",
    "    site_lats, site_lons = ut0.latlon2utm(insar_metadata, site_lats_wgs84, site_lons_wgs84)\n",
    "else:\n",
    "    site_lats = site_lats_wgs84\n",
    "    site_lons = site_lons_wgs84\n",
    "site_names = [str(stn) for stn in site_names]\n",
    "print(\"Initial list of {} stations used in analysis:\".format(len(site_names)))\n",
    "print(site_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_gps_ts'></a>\n",
    "## 4.3. Get GNSS Position Time Series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get daily position solutions for GNSS stations\n",
    "use_stn = []  #stations to keep\n",
    "bad_stn = []  #stations to toss\n",
    "use_lats = [] \n",
    "use_lons = []\n",
    "# track latlon coordinates for UTM grids\n",
    "use_lats_keepwgs84 = [] \n",
    "use_lats_keepwgs84 = []\n",
    "\n",
    "for counter, stn in enumerate(site_names):\n",
    "    gps_obj = GNSS(site = stn,\n",
    "                   data_dir = os.path.join(mintpy_dir,f'GNSS-{gnss_source}'))\n",
    "    gps_obj.open(print_msg=False)\n",
    "    \n",
    "    # count number of dates in time range\n",
    "    dates = gps_obj.dates\n",
    "    range_days = (end_date_gnss - start_date_gnss).days\n",
    "    gnss_count = np.histogram(dates, bins=[start_date_gnss, end_date_gnss])\n",
    "    gnss_count = int(gnss_count[0])\n",
    "    \n",
    "    # for this quick screening check of data quality, we use the constant incidence and azimuth angles \n",
    "    # get standard deviation of residuals to linear fit\n",
    "    disp_los = ut.enu2los(gps_obj.dis_e, gps_obj.dis_n, gps_obj.dis_u, inc_angle, az_angle)\n",
    "    disp_detrended = signal.detrend(disp_los)\n",
    "    stn_stdv = np.std(disp_detrended)\n",
    "   \n",
    "    # select GNSS stations based on data completeness and scatter of residuals\n",
    "    disp_detrended = signal.detrend(disp_los)\n",
    "    if range_days * gps_completeness_threshold <= gnss_count:\n",
    "        if stn_stdv > gps_residual_stdev_threshold:\n",
    "            bad_stn.append(stn)\n",
    "        else:\n",
    "            use_stn.append(stn)\n",
    "            use_lats.append(site_lats[counter])\n",
    "            use_lons.append(site_lons[counter])\n",
    "            use_lats_keepwgs84.append(site_lats_wgs84[counter])\n",
    "            use_lats_keepwgs84.append(site_lons_wgs84[counter])\n",
    "    else:\n",
    "        bad_stn.append(stn)\n",
    "\n",
    "site_names = use_stn\n",
    "site_lats = use_lats\n",
    "site_lons = use_lons\n",
    "site_lats_wgs84 = use_lats_keepwgs84\n",
    "site_lons_wgs84 = use_lats_keepwgs84\n",
    "\n",
    "# [optional] manually remove additional stations\n",
    "gnss_to_remove=[]\n",
    "\n",
    "for i, gnss_site in enumerate(gnss_to_remove):\n",
    "    if gnss_site in site_names:\n",
    "        site_names.remove(gnss_site)\n",
    "    if gnss_site not in bad_stn:\n",
    "        bad_stn.append(gnss_site)\n",
    "\n",
    "print(\"\\nFinal list of {} stations used in analysis:\".format(len(site_names)))\n",
    "print(site_names)\n",
    "print(\"List of {} stations removed from analysis\".format(len(bad_stn)))\n",
    "print(bad_stn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_gps_los'></a>\n",
    "## 4.4. Project GNSS to LOS Velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gnss_velocities = gnss.get_los_obs(insar_metadata, \n",
    "                            'velocity', \n",
    "                            site_names, \n",
    "                            start_date=start_date,\n",
    "                            end_date=end_date,\n",
    "                            source=gnss_source,\n",
    "                            gnss_comp='enu2los', \n",
    "                            redo=True)\n",
    "\n",
    "# scale site velocities from m/yr to mm/yr\n",
    "gnss_velocities *= 1000.\n",
    "\n",
    "print('\\n site   vel_los [mm/yr]')\n",
    "print(np.array([site_names, gnss_velocities]).T)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_gps_insar'></a>\n",
    "## 4.5. Re-Reference GNSS and InSAR LOS Velocities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reference GNSS stations to GNSS reference site\n",
    "ref_site_ind = site_names.index(sites[site]['gps_ref_site_name'])\n",
    "gnss_velocities = gnss_velocities - gnss_velocities[ref_site_ind]\n",
    "\n",
    "# reference InSAR to GNSS reference site\n",
    "ref_site_lat = float(site_lats[ref_site_ind])\n",
    "ref_site_lon = float(site_lons[ref_site_ind])\n",
    "ref_y, ref_x = ut.coordinate(insar_metadata).geo2radar(ref_site_lat, ref_site_lon)[:2]\n",
    "if not math.isnan(insar_velocities[ref_y, ref_x]):\n",
    "    #insar_velocities = insar_velocities - insar_velocities[ref_y, ref_x]\n",
    "    #Caution: If you expand the radius parameter farther than the bounding grid it will break. \n",
    "    #To fix, remove the station in section 4 when the site_names list is filtered\n",
    "    ref_vel_px_rad = insar_velocities[ref_y-pixel_radius:ref_y+1+pixel_radius, \n",
    "                        ref_x-pixel_radius:ref_x+1+pixel_radius]\n",
    "    ref_insar_site_vel = np.nanmedian(ref_vel_px_rad)\n",
    "    if np.isnan(ref_insar_site_vel):\n",
    "        ref_insar_site_vel = 0.\n",
    "    insar_velocities = insar_velocities - ref_insar_site_vel\n",
    "\n",
    "# plot GNSS stations on InSAR velocity field\n",
    "vmin = sites[site]['vmin']\n",
    "vmax = sites[site]['vmax']\n",
    "cmap = plt.get_cmap('RdBu_r')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "cax = ax.imshow(insar_velocities, cmap=cmap, vmin=vmin, vmax=vmax,\n",
    "                interpolation='nearest', extent=(OG_W, OG_E, OG_S, OG_N))\n",
    "cbar = fig.colorbar(cax, ax=ax)\n",
    "cbar.set_label('LOS velocity [mm/year]')\n",
    "\n",
    "for lat, lon, obs in zip(site_lats, site_lons, gnss_velocities):\n",
    "    color = cmap((obs - vmin)/(vmax - vmin))\n",
    "    ax.scatter(lon, lat, color=color, s=8**2, edgecolors='k')\n",
    "for i, label in enumerate(site_names):\n",
    "     plt.annotate(label, (site_lons[i], site_lats[i]), color='black')\n",
    "\n",
    "out_fig = os.path.abspath('vel_insar_vs_gnss.png')\n",
    "fig.savefig(out_fig, bbox_inches='tight', transparent=True, dpi=300)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_nisar_validation'></a>\n",
    "# 5. NISAR Validation Approach 1: GNSS-InSAR Direct Comparison \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_make_vel'></a>\n",
    "## 5.1. Make Velocity Residuals at GNSS Locations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create dictionary with the stations as the key and all their info as an array \n",
    "stn_dict = {}\n",
    "\n",
    "#Loop over GNSS station locations\n",
    "for i in range(len(site_names)): \n",
    "    # convert GNSS station lat/lon information to InSAR x/y grid\n",
    "    stn_lat = float(site_lats[i])\n",
    "    stn_lon = float(site_lons[i])\n",
    "    y_value, x_value = ut.coordinate(insar_metadata).geo2radar(stn_lat, stn_lon)[:2]\n",
    "    # post-query: convert lat/lon to UTM\n",
    "    if 'UTM_ZONE' in insar_metadata.keys():\n",
    "        stn_lat = float(site_lats_wgs84[i])\n",
    "        stn_lon = float(site_lons_wgs84[i])\n",
    "    \n",
    "    # get velocities and residuals\n",
    "    gnss_site_vel = gnss_velocities[i]\n",
    "    #Caution: If you expand the radius parameter farther than the bounding grid it will break. \n",
    "    #To fix, remove the station in section 4 when the site_names list is filtered\n",
    "    vel_px_rad = insar_velocities[y_value-pixel_radius:y_value+1+pixel_radius, \n",
    "                     x_value-pixel_radius:x_value+1+pixel_radius]\n",
    "    insar_site_vel = np.nanmedian(vel_px_rad)\n",
    "    if np.isnan(insar_site_vel):\n",
    "        insar_site_vel = 0.\n",
    "    residual = gnss_site_vel - insar_site_vel\n",
    "\n",
    "    # populate data structure\n",
    "    values = [x_value, y_value, insar_site_vel, gnss_site_vel, residual, stn_lat, stn_lon]\n",
    "    stn = site_names[i]\n",
    "    stn_dict[stn] = values\n",
    "\n",
    "# extract data from structure\n",
    "res_list = []\n",
    "insar_site_vels = []\n",
    "gnss_site_vels = []\n",
    "lat_list = []\n",
    "lon_list = []\n",
    "for i in range(len(site_names)): \n",
    "    stn = site_names[i]\n",
    "    insar_site_vels.append(stn_dict[stn][2])\n",
    "    gnss_site_vels.append(stn_dict[stn][3])\n",
    "    res_list.append(stn_dict[stn][4])\n",
    "    lat_list.append(stn_dict[stn][5])\n",
    "    lon_list.append(stn_dict[stn][6])\n",
    "num_stn = len(site_names) \n",
    "print('Finish creating InSAR residuals at GNSS sites')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_make_velres'></a>\n",
    "## 5.2. Make Double-Differenced Velocity Residuals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_gps_sites = len(site_names)\n",
    "diff_res_list = []\n",
    "stn_dist_list = []\n",
    "\n",
    "# loop over stations\n",
    "for i in range(n_gps_sites-1):\n",
    "    stn1 = site_names[i]\n",
    "    for j in range(i + 1, n_gps_sites):\n",
    "        stn2 = site_names[j]\n",
    "\n",
    "        # calculate GNSS and InSAR velocity differences between stations\n",
    "        gps_vel_diff = stn_dict[stn1][3] - stn_dict[stn2][3]\n",
    "        insar_vel_diff = stn_dict[stn1][2] - stn_dict[stn2][2]\n",
    "\n",
    "        # calculate GNSS vs InSAR differences (double differences) between stations\n",
    "        diff_res = gps_vel_diff - insar_vel_diff\n",
    "        diff_res_list.append(diff_res)\n",
    "\n",
    "        # get distance (km) between stations using Haversine formula\n",
    "        # index 5 is lat, 6 is lon\n",
    "        stn_dist = haversine_distance(stn_dict[stn1][6], stn_dict[stn1][5], stn_dict[stn2][6], stn_dict[stn2][5])\n",
    "        stn_dist_list.append(stn_dist)\n",
    "\n",
    "# Write data for statistical tests\n",
    "gnss_site_dist = np.array(stn_dist_list)\n",
    "double_diff_rel_measure = np.array(np.abs(diff_res_list))\n",
    "ndx = np.argsort(gnss_site_dist)\n",
    "\n",
    "# Plot data to be used below\n",
    "fig, ax = plt.subplots(figsize=[11, 7])\n",
    "plt.scatter(gnss_site_dist, diff_res_list, label='V_gnss - V_InSAR for station pair')\n",
    "plt.axhline(secular_gnss_rqmt, color='r', linestyle='--', label='Secular rqmt')\n",
    "plt.axhline(-1*secular_gnss_rqmt, color='r', linestyle='--', label='Secular rqmt')\n",
    "plt.ylim(-10,10)\n",
    "plt.xlim(*gnss_dist_rqmt)\n",
    "plt.legend(loc='upper left')\n",
    "plt.title(f\"Double-Difference Residuals \\n Date range {start_date}-{end_date} \\n GNSS-InSAR velocities\")\n",
    "plt.xlabel(\"Distance (km)\")\n",
    "plt.ylabel(\"Double-Differenced Velocity Residual (mm/y)\")\n",
    "plt.show()\n",
    "\n",
    "out_fig = os.path.abspath('secular_insar-gnss_velocity_vs_distance.png')\n",
    "fig.savefig(out_fig, bbox_inches='tight', transparent=True, dpi=300)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Final result Method 1—Successful when 68% of points below requirements line\n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_valid_method1'></a>\n",
    "## 5.3. Secular Requirement Validation: Method 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Statistics\n",
    "n_bins = 10\n",
    "threshold = 0.683\n",
    "\n",
    "if findMax == 'true':\n",
    "    thresh_flag = 'false'\n",
    "else :\n",
    "    thresh_flag = 'true'\n",
    "\n",
    "tmp_secular_gnss_rqmt = deepcopy(secular_gnss_rqmt)\n",
    "sucess_flag = thresh_flag\n",
    "\n",
    "#  we assume that the distribution of residuals is Gaussian and \n",
    "#  that the threshold represents a 1-sigma limit within which \n",
    "#  we expect 68.3% of residuals to lie.\n",
    "\n",
    "# define bins and data columns, the final column is the ratio as a whole\n",
    "bins = np.linspace(*gnss_dist_rqmt, num=n_bins+1)\n",
    "n_all = np.empty((n_bins+1), dtype=int) # number of points in each bin\n",
    "n_pass = np.empty((n_bins+1), dtype=int) # number of points that pass criterion\n",
    "ratio = np.empty((n_bins+1), dtype=float) # ratio of points that pass criterion\n",
    "\n",
    "# populate bins\n",
    "inds = np.digitize(gnss_site_dist, bins)\n",
    "while sucess_flag == thresh_flag:\n",
    "    for i in range(n_bins):\n",
    "        # relative measurement\n",
    "        gnss_rem = double_diff_rel_measure[inds == i+1]\n",
    "        n_all[i] = np.count_nonzero(~np.isnan(gnss_rem))\n",
    "        n_pass[i] = np.count_nonzero(gnss_rem < tmp_secular_gnss_rqmt)\n",
    "        if n_all[i] == 0:\n",
    "            ratio[i] = 1.000  # assume pass if no data fall in bin\n",
    "        else:\n",
    "            ratio[i] = n_pass[i]/n_all[i]\n",
    "\n",
    "    # fill in last column\n",
    "    n_all[-1] = np.sum(n_all[0:-1])\n",
    "    n_pass[-1] = np.sum(n_pass[0:-1])\n",
    "    ratio[-1] = n_pass[-1]/n_all[-1]\n",
    "\n",
    "    # determine success or failure for each bin\n",
    "    success_or_fail = ratio > threshold  # boolean array\n",
    "    success_or_fail_str = np.array([['true' if x==True else 'false' for x in success_or_fail]])\n",
    "\n",
    "    # build pandas table\n",
    "    columns = []\n",
    "    for i in range(n_bins):\n",
    "        columns.append(f'{bins[i]:.2f}-{bins[i+1]:.2f}')\n",
    "    columns.append('total')\n",
    "\n",
    "    index = ['-'.join([start_date, end_date])]\n",
    "\n",
    "    # Display Results\n",
    "    n_all_pd = pd.DataFrame(n_all.reshape(1,n_bins+1),columns=columns,index=index)\n",
    "    n_pass_pd = pd.DataFrame(n_pass.reshape(1,n_bins+1),columns=columns,index=index)\n",
    "    ratio_pd = pd.DataFrame(ratio.reshape(1,n_bins+1),columns=columns,index=index)\n",
    "    success_or_fail_pd = pd.DataFrame(success_or_fail_str.reshape(1,n_bins+1),columns=columns,index=index)\n",
    "\n",
    "    #display(n_all_pd)  # Number of data points in each bin\n",
    "    #display(n_pass_pd) # Number of data points that lie below the curve\n",
    "\n",
    "    #Set new style for table\n",
    "    s = ratio_pd.style\n",
    "    s.set_table_styles([  # create internal CSS classes\n",
    "        {'selector': '.true', 'props': 'background-color: #e6ffe6;'},\n",
    "        {'selector': '.false', 'props': 'background-color: #ffe6e6;'},\n",
    "    ], overwrite=False)\n",
    "    #display(s.set_td_classes(success_or_fail_pd))  # Percentage of passing points:\n",
    "    #display(success_or_fail_pd)  # Explicit pass/fail table\n",
    "    sucess_flag = success_or_fail_pd.iloc[0]['total']\n",
    "\n",
    "    if findMax == 'true' :\n",
    "        tmp_secular_gnss_rqmt += 0.01\n",
    "    else :\n",
    "        tmp_secular_gnss_rqmt -= 0.01\n",
    "\n",
    "display(n_all_pd)  # Number of data points in each bin\n",
    "display(n_pass_pd) # Number of data points that lie below the curve\n",
    "display(s.set_td_classes(success_or_fail_pd))  # Percentage of passing points:\n",
    "display(success_or_fail_pd)  # Explicit pass/fail table\n",
    "\n",
    "print(tmp_secular_gnss_rqmt, success_or_fail_pd.iloc[0]['total'])\n",
    "# Overall pass/fail criterion\n",
    "if success_or_fail_pd.iloc[0]['total'] == 'true':\n",
    "    print(\"This velocity dataset passes the requirement.\")\n",
    "elif success_or_fail_pd.iloc[0]['total'] == 'false':\n",
    "    print(\"This velocity dataset does not pass the requirement.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Final result Method 1 table by distance bin—successful when greater than 0.683\n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='secular_nisar_validation2'></a>\n",
    "# 6. NISAR Validation Approach 2: InSAR-only Structure Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Validation approach 2, we use a time interval and area where we assume no deformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot velocity map\n",
    "scp_args = 'velocity.h5 velocity -v -20 20 --colormap RdBu_r --figtitle LOS_Velocity --unit mm/yr -m ' + msk_file\n",
    "view.main(scp_args.split())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='secular_array_mask'></a>\n",
    "## 6.1. Read Array and Mask Pixels with no Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the assumed non-earthquake displacement as the insar_displacment for statistics and convert to mm\n",
    "insar_velocities,_ = readfile.read(vel_file, datasetName = 'velocity')  #read velocity\n",
    "velStart = sites[site]['download_start_date']\n",
    "insar_velocities = insar_velocities * 1000.  # convert velocities from m to mm\n",
    "\n",
    "# set masked pixels to NaN\n",
    "msk,_ = readfile.read(msk_file)\n",
    "insar_velocities[msk == 0] = np.nan\n",
    "insar_velocities[insar_velocities == 0] = np.nan\n",
    "\n",
    "# display map of data after masking\n",
    "cmap = plt.get_cmap('RdBu_r')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "img1 = ax.imshow(insar_velocities, cmap=cmap, vmin=-20, vmax=20, interpolation='nearest')\n",
    "ax.set_title(\"Secular \\n Date \"+velStart)\n",
    "cbar1 = fig.colorbar(img1, ax=ax)\n",
    "cbar1.set_label('LOS velocity [mm/year]')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='secular_remove_trend'></a>\n",
    "## 6.2. Randomly Sample Pixels and Pair Them Up with Option to Remove Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_mode = 'points'  # 'points' or 'profile'\n",
    "# note that the 'profile' method may take significantly longer\n",
    "\n",
    "# Collect samples using the specified method\n",
    "if sample_mode in ['points']:\n",
    "    if 'UTM_ZONE' in insar_metadata.keys():\n",
    "        X0,Y0 = load_geo_utm(insar_metadata)\n",
    "    else:\n",
    "        X0,Y0 = load_geo(insar_metadata)\n",
    "    X0_2d,Y0_2d = np.meshgrid(X0,Y0)\n",
    "\n",
    "    insar_sample_dist, insar_rel_measure = samp_pair(X0_2d, Y0_2d, insar_velocities, num_samples=1000000)\n",
    "\n",
    "elif sample_mode in ['profile']:\n",
    "    # access metadata\n",
    "    length, width = int(insar_metadata['LENGTH']), int(insar_metadata['WIDTH'])\n",
    "    # if UTM, must adjust input metadata to reflect lat/lon coordinates\n",
    "    if 'UTM_ZONE' in insar_metadata.keys():\n",
    "        # recompute steps\n",
    "        X_utm = np.linspace(OG_W+lon_step, OG_E-lon_step, width)  # longitudes\n",
    "        Y_utm = np.linspace(OG_N+lat_step, OG_S-lat_step, length)  # latitudes\n",
    "        lat1, lon1 = ut0.utm2latlon(insar_metadata, X_utm[0], Y_utm[0])\n",
    "        lat2, _ = ut0.utm2latlon(insar_metadata, X_utm[0], Y_utm[1])\n",
    "        _, lon2 = ut0.utm2latlon(insar_metadata, X_utm[1], Y_utm[0])\n",
    "        lat_step = lat2 - lat1\n",
    "        lon_step = lon2 - lon1\n",
    "        # recompute ref coordinates\n",
    "        ref_lat_wgs84, ref_lon_wgs84 = ut0.utm2latlon(insar_metadata,\n",
    "            float(insar_metadata['REF_LON']), float(insar_metadata['REF_LAT']))\n",
    "        # recompute starting coordinates\n",
    "        y_first, x_first = ut0.utm2latlon(insar_metadata,\n",
    "            float(insar_metadata['X_FIRST']), float(insar_metadata['Y_FIRST']))\n",
    "        # reassign wgs84 values\n",
    "        insar_metadata['REF_LAT'] = str(ref_lat_wgs84)\n",
    "        insar_metadata['REF_LON'] = str(ref_lon_wgs84)\n",
    "        insar_metadata['Y_STEP'] = str(lat_step)\n",
    "        insar_metadata['X_STEP'] = str(lon_step)\n",
    "        insar_metadata['Y_FIRST'] = str(N) #str(y_first)\n",
    "        insar_metadata['X_FIRST'] = str(W) #str(x_first)\n",
    "        # delete references to UTM coordinate system\n",
    "        insar_metadata['Y_UNIT'] = 'degrees'\n",
    "        insar_metadata['X_UNIT'] = 'degrees'\n",
    "        insar_metadata['EPSG'] = '4326'\n",
    "        insar_metadata.pop('UTM_ZONE')\n",
    "\n",
    "    X = np.linspace(W+lon_step, E-lon_step, width)  # longitudes\n",
    "    Y = np.linspace(N+lat_step, S-lat_step, length)  # latitudes\n",
    "    X_coords, Y_coords = np.meshgrid(X, Y)\n",
    "\n",
    "    # Draw random samples from map (without replacement)\n",
    "    num_samples = 20000\n",
    "    \n",
    "    # Retrieve profile samples\n",
    "    insar_sample_dist, insar_rel_measure = profile_samples(\\\n",
    "                    x=X_coords.reshape(-1,1),\n",
    "                    y=Y_coords.reshape(-1,1),\n",
    "                    data=insar_velocities,\n",
    "                    metadata=insar_metadata,\n",
    "                    len_rqmt=insar_dist_rqmt,\n",
    "                    num_samples=num_samples)\n",
    "\n",
    "print('Finished sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "img1 = ax.hist(insar_sample_dist, bins=100)\n",
    "ax.set_title(\"Histogram of distance \\n Secular Date {:s} - {:s}\".format(start_date, end_date))\n",
    "ax.set_xlabel(r'Distance ($km$)')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_xlim(*insar_dist_rqmt)\n",
    "    \n",
    "fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "img1 = ax.hist(insar_rel_measure, bins=100)\n",
    "ax.set_title(\"Histogram of Relative Measurement \\n Secular Date {:s} - {:s}\".format(start_date, end_date))\n",
    "ax.set_xlabel(r'Relative Measurement ($mm/year$)')\n",
    "ax.set_ylabel('Frequency')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='secular_M2ampvsdist2'></a>\n",
    "## 6.3. Amplitude vs. Distance of Relative Measurements (pair differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[18, 7.5])\n",
    "plt.axhline(secular_insar_rqmt, color='r', linestyle='--', label='Secular rqmt')\n",
    "ax.scatter(insar_sample_dist, insar_rel_measure, s=1, alpha=0.25, label='Relative velocity for pixel pair')\n",
    "ax.set_title(f\"Method 2: Relative Velocity Measurements between Pixel Pairs and Requirement Curve vs. Distance \\n {site} Secular Date range {start_date}-{end_date}\" )\n",
    "ax.set_ylabel(r'Relative Velocity Measurement ($mm/year$)')\n",
    "ax.set_xlabel('Distance (km)')\n",
    "ax.set_xlim(*insar_dist_rqmt)\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "out_fig = os.path.abspath('secular_insar-only_vs_distance_'+site+'_date'+velStart+'.png')\n",
    "fig.savefig(out_fig, bbox_inches='tight', transparent=True, dpi=300)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Final result Method 2—\n",
    "    68% of points below the requirements line is success\n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='secular_M2RelMeasTable'></a>\n",
    "## 6.4. Bin Sample Pairs by Distance Bin and Calculate Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Statistics\n",
    "n_bins = 10\n",
    "threshold = 0.683 \n",
    "\n",
    "if findMax == 'true':\n",
    "    thresh_flag = 'false'\n",
    "else :\n",
    "    thresh_flag = 'true'\n",
    "\n",
    "tmp_secular_insar_rqmt = deepcopy(secular_insar_rqmt)\n",
    "sucess_flag = thresh_flag\n",
    "\n",
    "#  we assume that the distribution of residuals is Gaussian and \n",
    "#  that the threshold represents a 1-sigma limit within which \n",
    "#  we expect 68.3% of residuals to lie.\n",
    "\n",
    "# define bins and data columns, the final column is the ratio as a whole\n",
    "bins = np.linspace(*insar_dist_rqmt, num=n_bins+1)\n",
    "n_all = np.empty((n_bins+1), dtype=int) # number of points in each bin\n",
    "n_pass = np.empty((n_bins+1), dtype=int) # number of points that pass criterion\n",
    "ratio = np.empty((n_bins+1), dtype=float) # ratio of points that pass criterion\n",
    "\n",
    "# populate bins\n",
    "inds = np.digitize(insar_sample_dist, bins)\n",
    "while sucess_flag == thresh_flag:\n",
    "    for i in range(n_bins):\n",
    "        # relative measurement\n",
    "        insar_rem = insar_rel_measure[inds == i+1]\n",
    "        n_all[i] = np.count_nonzero(~np.isnan(insar_rem))\n",
    "        n_pass[i] = np.count_nonzero(insar_rem < tmp_secular_insar_rqmt)\n",
    "        if n_all[i] == 0:\n",
    "            ratio[i] = 1.000  # assume pass if no data fall in bin\n",
    "        else:\n",
    "            ratio[i] = n_pass[i]/n_all[i]\n",
    "\n",
    "    # fill in last column\n",
    "    n_all[-1] = np.sum(n_all[0:-1])\n",
    "    n_pass[-1] = np.sum(n_pass[0:-1])\n",
    "    ratio[-1] = n_pass[-1]/n_all[-1]\n",
    "\n",
    "    # determine success or failure for each bin\n",
    "    success_or_fail = ratio > threshold  # boolean array\n",
    "    success_or_fail_str = np.array([['true' if x==True else 'false' for x in success_or_fail]])\n",
    "\n",
    "    # build pandas table\n",
    "    columns = []\n",
    "    for i in range(n_bins):\n",
    "        columns.append(f'{bins[i]:.2f}-{bins[i+1]:.2f}')\n",
    "    columns.append('total')\n",
    "\n",
    "    index = ['-'.join([start_date, end_date])]\n",
    "        \n",
    "    # Display Results\n",
    "    n_all_pd = pd.DataFrame(n_all.reshape(1,n_bins+1),columns=columns,index=index)\n",
    "    n_pass_pd = pd.DataFrame(n_pass.reshape(1,n_bins+1),columns=columns,index=index)\n",
    "    ratio_pd = pd.DataFrame(ratio.reshape(1,n_bins+1),columns=columns,index=index)\n",
    "    success_or_fail_pd = pd.DataFrame(success_or_fail_str.reshape(1,n_bins+1),columns=columns,index=index)\n",
    "\n",
    "    #display(n_all_pd)  # Number of data points in each bin\n",
    "    #display(n_pass_pd) # Number of data points that lie below the curve\n",
    "\n",
    "    #Set new style for table\n",
    "    s = ratio_pd.style\n",
    "    s.set_table_styles([  # create internal CSS classes\n",
    "        {'selector': '.true', 'props': 'background-color: #e6ffe6;'},\n",
    "        {'selector': '.false', 'props': 'background-color: #ffe6e6;'},\n",
    "    ], overwrite=False)\n",
    "    #display(s.set_td_classes(success_or_fail_pd))  # Percentage of passing points:\n",
    "    #display(success_or_fail_pd)  # Explicit pass/fail table\n",
    "    sucess_flag = success_or_fail_pd.iloc[0]['total']\n",
    "    if findMax == 'true' :\n",
    "        tmp_secular_insar_rqmt += 0.01\n",
    "    else :\n",
    "        tmp_secular_insar_rqmt -= 0.01\n",
    "\n",
    "display(n_all_pd)  # Number of data points in each bin\n",
    "display(n_pass_pd) # Number of data points that lie below the curve\n",
    "display(s.set_td_classes(success_or_fail_pd))  # Percentage of passing points:\n",
    "display(success_or_fail_pd)  # Explicit pass/fail table\n",
    "\n",
    "print(tmp_secular_insar_rqmt, success_or_fail_pd.iloc[0]['total'])\n",
    "# Overall pass/fail criterion\n",
    "if success_or_fail_pd.iloc[0]['total'] == 'true':\n",
    "    print(\"This velocity dataset passes the requirement.\")\n",
    "elif success_or_fail_pd.iloc[0]['total'] == 'false':\n",
    "    print(\"This velocity dataset does not pass the requirement.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Final result Method 2 table of distance bins—\n",
    "    68% (0.683) of points below the requirements line is success\n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_appendix1'></a>\n",
    "# Appendix: Supplementary Comparisons and Plots"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_compare_raw'></a>\n",
    "## A.1. Compare Raw Velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vmin, vmax = -25, 25\n",
    "plt.figure(figsize=(11,7))\n",
    "plt.hist(insar_site_vels, range=[vmin, vmax], bins=50, color=\"green\", edgecolor='grey', label='V_InSAR')\n",
    "plt.hist(gnss_site_vels, range=[vmin, vmax], bins=50, color=\"orange\", edgecolor='grey', label='V_gnss', alpha=0.5)\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(f\"Velocities \\n Date range {start_date}-{end_date} \\n Reference stn: {sites[site]['gps_ref_site_name']} \\n Number of stations used: {num_stn}\")\n",
    "plt.xlabel('LOS Velocity (mm/year)')\n",
    "plt.ylabel('N Stations')\n",
    "plt.ylim(0,20)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_plot_vel'></a>\n",
    "## A.2. Plot Velocity Residuals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vmin, vmax = -10, 10\n",
    "plt.figure(figsize=(11,7))\n",
    "plt.hist(res_list, bins = 40, range=[vmin,vmax], edgecolor='grey', color=\"darkblue\", linewidth=1, label='V_gnss - V_InSAR (area average)')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(f\"Residuals \\n Date range {start_date}-{end_date} \\n Reference stn: {sites[site]['gps_ref_site_name']} \\n Number of stations used: {num_stn}\")\n",
    "plt.xlabel('Velocity Residual (mm/year)')\n",
    "plt.ylabel('N Stations')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_plot_velres'></a>\n",
    "## A.3. Plot Double Difference Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11,7))\n",
    "plt.hist(diff_res_list, range = [vmin, vmax],bins = 40, color = \"darkblue\",edgecolor='grey',label='V_gnss_(s1-s2) - V_InSAR_(s1-s2)')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(f\"Difference Residualts \\n Date range {start_date}-{end_date} \\n Reference stn: {sites[site]['gps_ref_site_name']} \\n Number of stations used: {num_stn}\")\n",
    "plt.xlabel('Double Differenced Velocity Residual (mm/year)')\n",
    "plt.ylabel('N Stations')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='secular_appendix_gps'></a>\n",
    "## A.4. GNSS Timeseries Plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# grab the time-series file used for time function estimation given the template setup\n",
    "template = readfile.read_template(os.path.join(mintpy_dir, 'smallbaselineApp.cfg'))\n",
    "template = ut.check_template_auto_value(template)\n",
    "\n",
    "# read the time-series file\n",
    "insar_ts, atr = readfile.read(insar_ts_file, datasetName='timeseries')\n",
    "mask = readfile.read(os.path.join(mintpy_dir, maskFile))[0]\n",
    "print(f'reading timeseries from file: {insar_ts_file}')\n",
    "\n",
    "# Get date list\n",
    "date_list = timeseries(insar_ts_file).get_date_list()\n",
    "num_date = len(date_list)\n",
    "date0, date1 = date_list[0], date_list[-1]\n",
    "insar_dates = ptime.date_list2vector(date_list)[0]\n",
    "\n",
    "# spatial reference\n",
    "coord = ut.coordinate(atr)\n",
    "ref_site = sites[site]['gps_ref_site_name']\n",
    "ref_gnss_obj = GNSS(site=ref_site,\n",
    "                    data_dir=os.path.join(mintpy_dir, f'GNSS-{gnss_source}'))\n",
    "ref_lat, ref_lon = ref_gnss_obj.get_site_lat_lon()\n",
    "ref_y, ref_x = coord.geo2radar(ref_lat, ref_lon)[:2]\n",
    "if not np.any(mask[ref_y-pixel_radius:ref_y+1+pixel_radius, ref_x-pixel_radius:ref_x+1+pixel_radius]):\n",
    "    raise ValueError(f'Given reference GNSS site ({ref_site}) '\n",
    "                     'is in masked-out unrelible region in InSAR! '\n",
    "                     'Change to a different site.')\n",
    "\n",
    "#Caution: If you expand the radius parameter farther than the bounding grid it will break. \n",
    "#To fix, remove the station in section 4 when the site_names list is filtered\n",
    "OG_ref_insar_dis = insar_ts[:, ref_y-pixel_radius:ref_y+1+pixel_radius, \n",
    "                            ref_x-pixel_radius:ref_x+1+pixel_radius]\n",
    "ref_insar_dis = np.zeros(len(OG_ref_insar_dis))\n",
    "for i in range(len(OG_ref_insar_dis)):\n",
    "    ts_med_slice = np.nanmedian(OG_ref_insar_dis[i])\n",
    "    if np.isnan(ts_med_slice):\n",
    "        ts_med_slice = 0.\n",
    "    ref_insar_dis[i] = ts_med_slice\n",
    "\n",
    "# Plot displacements and velocity timeseries at GNSS station locations\n",
    "num_site = len(site_names)\n",
    "prog_bar = ptime.progressBar(maxValue=num_site)\n",
    "for i, site_name in enumerate(site_names):\n",
    "    prog_bar.update(i+1, suffix=f'{site_name} {i+1}/{num_site}')\n",
    "\n",
    "    ## read data\n",
    "    # read GNSS\n",
    "    gnss_obj = GNSS(site=site_name,\n",
    "                    data_dir=os.path.join(mintpy_dir, f'GNSS-{gnss_source}'))\n",
    "    gnss_dates, gnss_dis, _, gnss_lalo = gnss_obj.get_los_displacement(\n",
    "        atr, start_date=date0, end_date=date1, ref_site=ref_site)[:4]\n",
    "    # shift GNSS to zero-mean in time [for plotting purpose]\n",
    "    gnss_dis -= np.nanmedian(gnss_dis)\n",
    "\n",
    "    # read InSAR\n",
    "    y, x = coord.geo2radar(gnss_lalo[0], gnss_lalo[1])[:2]\n",
    "    insar_dis = insar_ts[:, y, x] - ref_insar_dis\n",
    "    # apply a constant shift in time to fit InSAR to GNSS\n",
    "    comm_dates = sorted(list(set(gnss_dates) & set(insar_dates)))\n",
    "    if comm_dates:\n",
    "        insar_flag = [x in comm_dates for x in insar_dates]\n",
    "        gnss_flag = [x in comm_dates for x in gnss_dates]\n",
    "        insar_dis -= np.nanmedian(insar_dis[insar_flag] - gnss_dis[gnss_flag])\n",
    "\n",
    "    ## plot figure\n",
    "    if gnss_dis.size > 0 and np.any(~np.isnan(insar_dis)):\n",
    "        fig, ax = plt.subplots(figsize=(12, 3))\n",
    "        ax.axhline(color='grey',linestyle='dashed', linewidth=2)\n",
    "        ax.scatter(gnss_dates, gnss_dis*100, s=2**2, label=\"GNSS Daily Positions\")\n",
    "        ax.scatter(insar_dates, insar_dis*100, label=\"InSAR Positions\")\n",
    "        # axis format\n",
    "        ax.set_title(f\"Station Name: {site_name}\") \n",
    "        ax.set_ylabel('LOS displacement [cm]')\n",
    "        ax.legend()\n",
    "prog_bar.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
